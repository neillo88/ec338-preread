[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EC338 Pre-reading",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This e-book provides some pre-reading material for the EC338: Econometrics 2 - Microeconometrics module. In line with the Applied Microeconomics and Microeconometrics literatures, EC338 has evolved over time to focus on causal inference, the identification strategies that underpin various casual estimands, and their corresponding estimators.\nAs a result of this shift in focus, less time is given to more advanced material relating to the linear estimators used in this literature. As the relevant material is covered in EC226: Econometrics 1, you should be able to follow EC338 with a little revision and study. However, the change in notation may ‘trip you up’ and certain topics will be easier to understand with a richer understanding of linear estimators. For this reason, I have prepared this material as a brief (re-)introduction to ordinary least squares (OLS).\nIn EC338 we work extensively with dummy variables. As you know from EC226, estimating a linear regression model with discrete independent variables is relatively simple and the interpretation of the coefficient is typically straight-forward. However, the use of dummy variables requires a careful consideration of (perfect) collinearity and to understand collinearity (or rank conditions) it helps to think of data as a matrix, or system of column vectors. This becomes even more important when we start to consider models with various dimensions of fixed-effects.\nThese notes begin by revisiting the basic linear regression model and OLS estimator using vector notation. Next we revisit the properties of OLS, using this same notation, but without extensive proofs. Proofs should be available from a range of textbooks, including Wooldridge (2011). The material that will be least familiar to you will be the geometry of OLS. Here we will cover projection matrices and how they can be used to understand partitioned regression. Finally, we discuss dummy variables, their projections, and issues of colinearity.\nRemember, this is still the same material covered in EC226, just using vector notation. In many instances this simplifies the notation, as summations over \\(n\\) and/or \\(t\\) can be replaced with a simple inner product of vectors. For example, consider the average of random variable \\(Y_i\\),\n\\[\n\\bar{Y} = \\frac{1}{n}\\sum_{i=1}^{n}Y_i\n\\]\nIf we define two \\(n\\times 1\\) column vectors,\n\\[\n\\ell = \\begin{bmatrix}1 \\\\ 1 \\\\ \\vdots \\\\ 1\\end{bmatrix}\\qquad \\text{and}\\qquad Y = \\begin{bmatrix}Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\\end{bmatrix}\n\\]\nthen the inner produce of these vectors is,\n\\[\n\\langle\\ell,Y\\rangle = \\ell'Y = 1\\cdot Y_1+1\\cdot Y_2+\\dots+1\\cdot Y_n = \\sum_{i=1}^{n}Y_i\n\\]\nIn addition,\n\\[\n\\langle\\ell,\\ell\\rangle = \\ell'\\ell = 1\\cdot 1+1\\cdot 1+\\dots+1\\cdot 1 = \\sum_{i=1}^{n}1 = n\n\\]\nThus, the average can be expressed as,\n\\[\n(\\ell'\\ell)^{-1}\\ell'Y = \\frac{1}{n}\\sum_{i=1}^{n}Y_i\n\\]\nIt turns out, this linear transformation of \\(Y\\),\n\\[\nAY = (\\ell'\\ell)^{-1}\\ell'Y\n\\]\nhas a structure that is common to many linear estimators; a structure that derives from the projection matrix of the \\(\\ell\\)-vector: \\(P_{\\ell} =\\ell(\\ell'\\ell)^{-1}\\ell'\\) . We will discuss this in Chapter Chapter 4 when we explore the geometry of OLS.\nAt the end of this e-book is a compendium on linear algebra basics. I may reference these during the e-book. The compendium is not exhaustive and does not include proofs. Please consult a linear algebra text book for further reading if you require.\nThe material in this e-book will not be examined directly, but will assist with your understanding of the material covered during term."
  },
  {
    "objectID": "linear-reg.html#vector-notation",
    "href": "linear-reg.html#vector-notation",
    "title": "2  The Linear Regression Model",
    "section": "2.1 Vector notation",
    "text": "2.1 Vector notation\nMost undergraduate textbooks discuss data in terms of random variables: the dependent or outcome variable (typically denoted \\(Y_i\\)) and various independent or explanatory variables (\\(X_{i1},X_{i2},\\ldots,X_{ik}\\)). There’s nothing wrong with this language, but to understand the geometry of OLS we will need to think in terms of random vectors.\nWhen working with cross-sectional data, we think of a random sample as a collection of \\(n\\) realizations of the same random variable. Each observation represents a different unit (e.g., individual, firm, classroom, etc.) and we typically add the assumption that the data is i.i.d. (independently and identically distributed) across units of observation. It makes no difference then if we think of this random sample as a collection of \\(n\\times 1\\) random vectors, where each row represents a different unit and the unit of observation is maintained across random vector.\nFor each unit you observe the outcome (\\(Y_i\\)) and a vector of explanatory variables (or regressors):\n\\[\nX_i = \\begin{bmatrix}X_{i1} \\\\ X_{i2} \\\\ \\vdots \\\\ X_{ik}\\end{bmatrix}\n\\]\nTake note of the ordering of the subscripts: the first denotes the unit of observation (\\(i\\)) and the second the number of the regressor (\\(j=1\\dots k\\)). The pair \\((Y_i,X_i)\\) represents an observation, where \\(Y_i\\) is a single random variable and \\(X_i\\) a random (column) vector.1 A collection of observations forms a sample.\nYou are, no doubt, familiar with the linear regression model. A simple univariate model is typically written as,\n\\[\nY_i = \\beta_1 + \\beta_2 X_{i2} + \\varepsilon_i\n\\]\nWhere \\(\\beta_1\\) is the constant (intercept) and \\(\\beta_2\\) the slope coefficient. In EC338, we will discuss in more detail the justification for this model. For now, let us focus on notation.\nThe linear regression model is linear in parameters, which means we can express the outcome as a linear transformation of a finite set of parameters (i.e. \\(k\\times 1\\) vector of parameters). These (population) parameters are assumed to be constants and unknown to the econometrician.\nWe can rewrite the above equation using vectors,\n\\[\n\\begin{aligned} Y_i &= \\begin{bmatrix}1\\;X_{i2}\\end{bmatrix}\\begin{bmatrix}\\beta_1 \\\\ \\beta_2\\end{bmatrix} + \\varepsilon_i \\\\\n&= \\begin{bmatrix}1\\\\ X_{i2}\\end{bmatrix}'\\begin{bmatrix}\\beta_1 \\\\ \\beta_2\\end{bmatrix} + \\varepsilon_i \\\\\n&=X_i'\\beta + \\varepsilon_i\n\\end{aligned}\n\\]\nWhere \\(X_i\\) is a column vector including the number 1 in the first row (for the constant/intercept) and \\(X_{i2}\\) in the second row.\n\n[important] You may find my notation slightly unusual. A lot undergraduate textbooks use different letters (\\(\\alpha\\), \\(\\beta\\), \\(\\gamma\\), etc.) to denote the constant and slope coefficients. As we are collecting all coefficients in a single vector, it helps to use indexes instead of different letters.\nThen there is the issue of the where to start the index: \\(0\\) or \\(1\\). This decision is somewhat arbitrary and hinges on whether the the \\(k\\) regressors included in the model includes the constant term. At the top of the page I described an observation as an outcome and a random vector of regressors. As the constant is not random it is natural to think of it apart from the set of regressors included in the model. However, this decision is somewhat arbitrary. You could simply set \\(X_{i1} = 1\\;\\forall\\;i\\) and the constant would be included in \\(k\\).\nIn EC226, you indexed from 0. Recall, the linear model as \\(k+1\\) parameters; the \\(+1\\) for the constant. When computing the degrees of freedom for the RSS, you solved for \\(n-k-1\\): \\(k\\) regressors plus the constant.\nHere we will index from \\(1\\). This distinction makes it easier to keep track of the size of the matrix. It is also a more natural notation if you consider that the model need not have a constant. The choice of including a constant is therefore no different to including any other regressor. Moreover, when we consider models with fixed effects the constant typically drops from the model.\nThe key thing to remember is that you need to keep track of the number of parameters in the model, that includes the constant if there is one.\n\nWe can easily extend this notation to the case of multivariate regression. For example, consider a model with a constant and \\(k-1\\) regressors.\n\\[\n\\begin{aligned}\nY_i &= \\beta_1 + \\beta_2 X_{i2} + \\beta_3 X_{i3} + \\ldots + \\beta_k X_{ik} + \\varepsilon_i \\\\\n&= \\begin{bmatrix} 1\\; X_{i2}\\; X_{i3}\\; \\dots\\; X_{ik} \\end{bmatrix} \\begin{bmatrix} \\beta_1\\\\ \\beta_2\\\\ \\beta_3\\\\ \\vdots\\\\ \\beta_k\\end{bmatrix} + \\varepsilon_i \\\\\n&=X_i'\\beta + \\varepsilon_i\n\\end{aligned}\n\\]\nRegardless of the number of regressors, the notation remains the same. Take note of the fact that the \\(X_i\\) is a column vector which means that the notation must include a transpose: \\(X_i'\\beta\\). Excluding the transpose is incorrect since you cannot multiple two \\(k\\times1\\) column vectors. You can multiply a \\(1\\times k\\) row vector with a \\(k\\times 1\\) column vector giving you a \\(1\\times 1\\) scalar. The result should be a scalar since \\(Y_i\\) is a scalar.\nThis notation is not universal. For example, Wooldridge (2011) treats \\(X_i\\) as a row vector. For this reason, the linear regression model can be expressed as \\(X_i\\beta\\). Both notations are used in the applied literature, but I am more familiar with and prefer the column-vector notation."
  },
  {
    "objectID": "linear-reg.html#matrix-notation",
    "href": "linear-reg.html#matrix-notation",
    "title": "2  The Linear Regression Model",
    "section": "2.2 Matrix notation",
    "text": "2.2 Matrix notation\nThe above expressions for the linear regression model all describe a single unit of observation from the sample. Consider, each line included the subscript \\(i\\). Since the model is assumed to be the same for each observation, this an accurate depiction of the linear regression model. However, we also need to think about the correct notation for the entire sample. To do so, we will have to worker with both vectors and matrices.\nSince the model is the same for each observation in the sample, we could imagine “stacking” all \\(n\\) observations on top of one another to form a vector. Consider first the outcome variable,\n\\[\nY=\\begin{bmatrix}Y_1\\\\ Y_2 \\\\ \\vdots\\\\ Y_n \\end{bmatrix}\n\\]\n$$\n$$\n\\(Y\\) is a \\(n\\times 1\\) column vector of all outcomes in the sample. You can distinguish the vector \\(Y\\) from the scalar \\(Y_i\\) by the absence of a subscript.\nSimilarly, we can stack the right-hand side of the equation.\n\\[\n\\begin{aligned}\nY&=\\begin{bmatrix}X_1'\\beta\\\\ X_2'\\beta \\\\ \\vdots\\\\ X_n'\\beta \\end{bmatrix}+ \\begin{bmatrix}\\varepsilon_1\\\\ \\varepsilon_2 \\\\ \\vdots\\\\ \\varepsilon_n \\end{bmatrix}\\\\\n&=\\begin{bmatrix}X_1'\\\\ X_2' \\\\ \\vdots\\\\ X_n' \\end{bmatrix}\\beta+ \\begin{bmatrix}\\varepsilon_1\\\\ \\varepsilon_2 \\\\ \\vdots\\\\ \\varepsilon_n \\end{bmatrix}\\\\\n&=X\\beta+\\varepsilon\n\\end{aligned}\n\\]\nLike \\(Y\\), \\(\\varepsilon\\) is a \\(n\\times 1\\) vector. \\(X\\) is a \\(n\\times k\\) matrix and \\(\\beta\\) remains a \\(k\\times 1\\) vector of parameters. The product of a \\(n\\times k\\) matrix and \\(k\\times 1\\) vector is a \\(n\\times 1\\) vector: the same size vector as \\(Y\\) and \\(\\varepsilon\\). As it is important to understand the structure of \\(X\\), let us write it out in detail.\n\\[\nX = \\begin{bmatrix}X_1'\\\\ X_2' \\\\ \\vdots\\\\ X_n' \\end{bmatrix} = \\begin{bmatrix}X_{11} & X_{12}&\\dots&X_{1k}\\\\ X_{21}& X_{22}& & \\\\ \\vdots & & \\ddots &\\\\ X_{n1} & & & X_{nk} \\end{bmatrix}\n\\]\nThe \\(X\\) matrix has \\(n\\) rows, each representing a different unit of observation, and \\(k\\) columns, each representing a different regressor. Recall, one of these regressors may be a constant. If the model includes a constant then \\(X_{i1}=1\\;\\forall\\;i\\). This means that the first column of \\(X\\) is a vector of \\(1\\)’s. Each subsequent column represents another regressor.\nIf you are familiar with rectangular datasets from with working in STATA or R, then you may have notices that \\(X\\) is essentially the “dataset” (excluding the outcome variable). In a rectangular dataset, each row represents a different observation and each column a different variable. That’s what we have here.\nWhy is this noteworthy? When assert that there must be an absence of perfect colinearity between the variables in the model, we are actually saying that the columns of \\(X\\) must be linearly independent. The formal way of expressing this is that \\(X\\) must have full column rank; or \\(r(X)=k\\) (see Chapter 6 for a definition of linear dependence and rank). This is why the OLS condition included in EC226 as “no perfect colinearity” is sometimes referred to as the rank condition. Without full rank, we cannot estimate the linear regression model."
  },
  {
    "objectID": "linear-reg.html#footnotes",
    "href": "linear-reg.html#footnotes",
    "title": "2  The Linear Regression Model",
    "section": "",
    "text": "In these notes, as in the remainder of EC338, I treat \\(X_i\\) as a column vector. Some texts, including Wooldridge (2011), will treat \\(X_i\\) as a row vector. This distinction is not significant, but will affect your notation. I will point this out at a later stage.↩︎"
  },
  {
    "objectID": "ols.html#the-uni-variate-case",
    "href": "ols.html#the-uni-variate-case",
    "title": "3  Ordinary Least Squares",
    "section": "3.1 The uni-variate case",
    "text": "3.1 The uni-variate case\nUndergraduate textbooks all teach a very similar expression for the OLS estimator of a uni-variate regression model (with a constant), such as\n\\[\nY_i = \\beta_1+\\beta_2X_{i2}+\\varepsilon_i\n\\]\n\n[note] Once you are familiar with vector notation, it is relatively easy to tell whether a model is uni- or multi-variate. This is because the notation \\(\\beta_2 X_{i2}\\) is not consistent with \\(X_{2i}\\) being a vector (row or column).\nIf \\(X_{i2}\\) is a \\(k\\times 1\\) vector then so is \\(\\beta_2\\). Thus, \\(\\beta_2 X_{i2}\\) is \\((k\\times 1)\\cdot (k\\times1)\\), which is not defined.\nIf \\(X_{i2}\\) is a row vector (as in Wooldridge, 2011), \\(\\beta_2 X_{i2}\\) will then be \\((k\\times 1)\\cdot (1\\times k)\\), a \\(k\\times k\\) matrix. This cannot be correct since the model is defined at the unit level.\nThus, if you see a model written with the parameter in front of the regressor, you know that this must be a single regressor. This is subtle, yet imporant, distinction that researchers often use to convey the structure of their model. Whenever \\(X_{i2}\\) is a vector, researchers will almost always use the notation \\(X_{i2}'\\beta\\) or \\(X_{i2}\\beta\\), depending on whether \\(X_{i2}\\) is assumed to be a column or row vector.\n\nWe know that,\n\\[\n\\begin{aligned}\n\\tilde{\\beta}_2 =& \\frac{\\sum(Y_i-\\bar{Y})(X_{i2}-\\bar{X}_2)}{\\sum_{i=1}^n(X_{i2}-\\bar{X}_2)^2} \\\\\n\\text{and}\\qquad \\tilde{\\beta}_1 =& \\bar{Y}-\\tilde{\\beta_2}\\bar{X}_2\n\\end{aligned}\n\\]\nI am deliberately using the notation \\(\\tilde{\\beta}\\) to distinguish these two estimators from the expression below.\nLet us see if we can replicate this result. When written in vector notation, the model is,\n\\[\n\\begin{aligned}\nY =& X\\beta+\\varepsilon \\\\\n=& \\begin{bmatrix}1&X_{12} \\\\ 1 & X_{22} \\\\ \\vdots & \\vdots \\\\ 1 & X_{n2}\\end{bmatrix}\\begin{bmatrix}\\beta_1 \\\\ \\beta_2 \\end{bmatrix} + \\varepsilon \\\\\n=& \\begin{bmatrix}\\ell &X_{2} \\end{bmatrix}\\begin{bmatrix}\\beta_1 \\\\ \\beta_2 \\end{bmatrix} + \\varepsilon\n\\end{aligned}\n\\]\nTherefore,\n\\[\n\\begin{aligned}\n\\hat{\\beta} = \\begin{bmatrix}\\hat{\\beta}_1 \\\\ \\hat{\\beta}_2\\end{bmatrix}=&(X'X)^{-1}X'Y \\\\\n=&\\bigg(\\begin{bmatrix}\\ell' \\\\ X_{2}' \\end{bmatrix}\\begin{bmatrix}\\ell &X_{2}\\end{bmatrix}\\bigg)^{-1}\\begin{bmatrix}\\ell'  \\\\ X_{2}' \\end{bmatrix}Y \\\\\n=&\\begin{bmatrix}\\ell'\\ell & \\ell'X_2 \\\\ X_{2}'\\ell & X_2'X_2 \\end{bmatrix}^{-1}\\begin{bmatrix}\\ell'Y  \\\\ X_{2}'Y \\end{bmatrix}\n\\end{aligned}\n\\]\nI went through this rather quickly, using a number of linear algebra rules that you may not be familiar with. Do not worry, the point of the exercise is not become a linear algebra master, but instead to focus on the element of each of each matrix/vector. Each element is a scalar (size \\(1\\times 1\\)).\nIf we right them each down as sums you they might be a little more familiar. (Take a look back at Chapter 1 to remind yourself of some of these steps). First consider the \\(2\\times 2\\) matrix:\n\nelement [1,1]: \\(\\ell'\\ell = \\sum_{i=1}^n 1 = n\\)\nelement [1,2]: \\(\\ell'X_2 = \\sum_{i=1}^nX_{i2} = n\\bar{X}_2\\)\nelement [2,1]: \\(X_2'\\ell = \\sum_{i=1}^nX_{i2} = n\\bar{X}_2\\) (as above, since scalars are symmetric)\nelement [2,2]: \\(X_2'X_2=\\sum_{i=1}^nX_{i2}^2\\)\n\nNext, consider the final \\(2\\times 1\\) vector,\n\nelement [1,1]: \\(\\ell'Y = \\sum_{i=1}^n Y_i = n\\bar{Y}\\)\nelement [2,1]: \\(X_2'Y = \\sum_{i=1}^nY_iX_{i2}\\)\n\nOur OLS estimator is therefore,\n\\[\n\\hat{\\beta} = \\begin{bmatrix} n & n\\bar{X}_2 \\\\ n\\bar{X}_2 & \\sum_{i=1}^nX_{i2}^2 \\end{bmatrix}^{-1}\\begin{bmatrix}n\\bar{Y}_2  \\\\ \\sum_{i=1}^nY_iX_{i2} \\end{bmatrix}\n\\]\nWe now need to solve for the inverse of the \\(2\\times 2\\) matrix. You can easily find notes on how to do this online. Here, I will just provide the solution.\n\\[\n\\hat{\\beta} = \\frac{1}{n\\sum_{i=1}^nX_{i2}^2-n^2\\bar{X}_2^2}\\begin{bmatrix} \\sum_{i=1}^nX_{i2}^2 & -n\\bar{X}_2 \\\\ -n\\bar{X}_2 &  n\\end{bmatrix}\\begin{bmatrix}n\\bar{Y}_2  \\\\ \\sum_{i=1}^nY_iX_{i2} \\end{bmatrix}\n\\]\nRemember, this is still a \\(2\\times 1\\) vector. We can now solve for the final solution:\n\\[\n\\begin{aligned}\n\\hat{\\beta} =& \\frac{1}{n\\sum_{i=1}^nX_{i2}^2-n^2\\bar{X}_2^2}\\begin{bmatrix} n\\bar{Y}\\sum_{i=1}^nX_{i2}^2 -n\\bar{X}_2\\sum_{i=1}^nY_iX_{i2} \\\\ n\\sum_{i=1}^nY_iX_{i2}-n^2\\bar{X}_2\\bar{Y}\\end{bmatrix} \\\\\n=& \\frac{1}{n\\sum_{i=1}^n(X_{i2}-\\bar{X}_2)^2}\\begin{bmatrix} n\\bar{Y}\\sum_{i=1}^nX_{i2}^2 + n^2\\bar{Y}\\bar{X}^2 - n^2\\bar{Y}\\bar{X}^2 -n\\bar{X}_2\\sum_{i=1}^nY_iX_{i2} \\\\ n\\sum_{i=1}^n(Y_i-\\bar{Y})(X_{i2}-\\bar{X}_2) \\end{bmatrix} \\\\\n=& \\frac{1}{n\\sum_{i=1}^n(X_{i2}-\\bar{X}_2)^2}\\begin{bmatrix} n\\bar{Y}\\sum_{i=1}^n(X_{i2}-\\bar{X})^2 -n\\bar{X}_2\\sum_{i=1}^n(Y_i-\\bar{Y})(X_{i2}-\\bar{X}_2) \\\\ n\\sum_{i=1}^n(Y_i-\\bar{Y})(X_{i2}-\\bar{X}_2) \\end{bmatrix} \\\\\n=& \\begin{bmatrix} \\bar{Y}  -\\frac{n\\sum_{i=1}^n(Y_i-\\bar{Y})(X_{i2}-\\bar{X}_2)}{n\\sum_{i=1}^n(X_{i2}-\\bar{X}_2)^2}\\bar{X}_2 \\\\ \\frac{n\\sum_{i=1}^n(Y_i-\\bar{Y})(X_{i2}-\\bar{X}_2)}{n\\sum_{i=1}^n(X_{i2}-\\bar{X}_2)^2} \\end{bmatrix} \\\\\n=& \\begin{bmatrix} \\bar{Y}  -\\tilde{\\beta}_2\\bar{X}_2 \\\\ \\tilde{\\beta}_2 \\end{bmatrix} \\\\\n=& \\begin{bmatrix}\\tilde{\\beta}_1 \\\\ \\tilde{\\beta}_2 \\end{bmatrix}\n\\end{aligned}\n\\]\nThe math is a little involved, but it shows you these solutions are are the same. Unfortunately, the working gets even more arduous in a multivariate context. However, there are useful tools to help us with that we will discuss next."
  },
  {
    "objectID": "geometry-ols.html#partitioned-regression",
    "href": "geometry-ols.html#partitioned-regression",
    "title": "4  The Geometry of OLS",
    "section": "4.1 Partitioned regression",
    "text": "4.1 Partitioned regression\nThe tools of linear algebra can help us better understand partitioned regression. Indeed, I would go as far to to say that it is quite difficult to understand partitioned regression without an understanding of projection matrices. Moreover, we need to understand partitioned regression to really understand multivariate regression.\nLet us divide the set of regressors into two groups: \\(X_1\\) a single regressor and \\(X_2\\) a \\(n\\times (k-1)\\) matrix. We can rewrite the linear model as,\n\\[\nY = X\\beta+ \\varepsilon =\\beta_1X_1+X_2\\beta_2+\\varepsilon\n\\]\nPartitioned regression is typically taught as follows. The OLS estimator for \\(\\beta_1\\) can achieved by first regressing \\(X_1\\) on \\(X_2\\),\n\\[\nX_1 = X_2\\gamma_2+\\upsilon_1\n\\]\nNext, you regress \\(Y\\) on the residual from the above model,\n\\[\nY = \\gamma_1 \\hat{\\upsilon}_1+\\xi\n\\]\nThe partitioned regression result states that the OLS estimator for \\(\\hat{\\gamma_1}=\\hat{\\beta_1}\\). This aids in our understanding of \\(\\beta_1\\) as the partial effect of \\(X_1\\) on \\(Y\\), holding \\(X_2\\) constant.\nWe can show this using projection matrices. Let us begin by applying our existing knowledge. From above, we know that the residual from the regression of \\(X_1\\) on \\(X_2\\) is,\n\\[\n\\hat{\\upsilon} = M_2X_1\n\\]\nwhere \\(M_2 = I_n-X_2(X_2'X_2)^{-1}X_2'\\). Thus, the model we estimate in the second step, is\n\\[\nY = \\gamma_1M_2X_1+\\xi\n\\]\nWe know that \\(\\hat{\\gamma}_1 = (\\hat{\\upsilon}'\\hat{\\upsilon})^{-1}\\hat{\\upsilon}'Y\\). Replacing the value of the residual, we get&gt; [note] We use both the symmetry and idempotent quality of \\(M_2\\).\n\\[\n\\begin{aligned}\n\\hat{\\gamma}_1 =& (\\hat{\\upsilon}'\\hat{\\upsilon})^{-1}\\hat{\\upsilon}'Y \\\\\n=&(X_1'M_2M_2X_1)^{-1}X_1'M_2Y \\\\\n=&(X_1'M_2X_1)^{-1}X_1'M_2Y\n\\end{aligned}\n\\]\nNext we want to show that \\(\\hat{\\beta}_1\\) is given by the same value. This part is more complicated. Let’s start with by reminding ourselves of the following:\n\\[\n\\begin{aligned}\nX'X\\hat{\\beta} &= X'Y \\\\\n\\begin{bmatrix}X_1 & X_2\\end{bmatrix}'\\begin{bmatrix}X_1 & X_2\\end{bmatrix}\\begin{bmatrix}\\hat{\\beta}_1 \\\\ \\hat{\\beta}_2\\end{bmatrix} &= \\begin{bmatrix}X_1 & X_2\\end{bmatrix}'Y \\\\\n\\begin{bmatrix}X_1'X_1 & X_1'X_2 \\\\ X_2'X_1 & X_2'X_2 \\end{bmatrix}\\begin{bmatrix}\\hat{\\beta}_1 \\\\ \\hat{\\beta}_2\\end{bmatrix} &= \\begin{bmatrix}X_1'Y \\\\ X_2'Y\\end{bmatrix}\n\\end{aligned}\n\\]\nWe could solve for \\(\\hat{\\beta}_1\\) by solving for the inverse of \\(X'X\\); however, this will take a long time. An easier approach is to simply verify that \\(\\hat{\\beta}_1=(X_1'M_2X_1)^{-1}X_1'M_2Y\\). Recall, \\(\\hat{\\beta}\\) splits \\(Y\\) into two components:\n\\[\nY = \\hat{\\beta}_1X_1+X_2\\hat{\\beta}_2 + \\hat{\\varepsilon}\n\\]\nIf we plug this definition of \\(Y\\) into the above expression we get,\n\\[\n\\begin{aligned}\n&(X_1'M_2X_1)^{-1}X_1'M_2(\\hat{\\beta}_1X_1+X_2\\hat{\\beta}_2 + \\hat{\\varepsilon}) \\\\\n=&\\hat{\\beta}_1\\underbrace{(X_1'M_2X_1)^{-1}X_1'M_2X_1}_{=I_n} \\\\\n&+\\underbrace{(X_1'M_2X_1)^{-1}X_1'M_2X_2\\hat{\\beta_2}}_{=0} \\\\\n&+\\underbrace{(X_1'M_2X_1)^{-1}X_1'M_2\\hat{\\varepsilon}}_{=0} \\\\\n=&\\hat{\\beta}_1\n\\end{aligned}\n\\]\nIn line 2, I use the fact that \\(\\hat{\\beta}_1\\) is a scalar and can be moved to the front (since the order of multiplication does not matter with a scalar). In line 3, I use the fact that \\(M_2X_2=0\\) by definition. Line 4 uses the fact that \\(M_2\\hat{\\varepsilon}=\\hat{\\varepsilon}\\) which means that \\(X_1'M_2\\hat{\\varepsilon}=X_1'\\hat{\\varepsilon}=0\\).\nThe OLS estimator solves for \\(\\beta_1\\) using the variance in \\(X_1\\) that is orthogonal to \\(X_2\\). This is the manner in which we “hold \\(X_2\\) constant”: the variation in \\(M_2X_1\\) is orthogonal to \\(X_2\\). Changes in \\(M_2X_1\\) are uncorrelated with changes in \\(X_2\\); as if the variation in \\(M_2X_1\\) arose independently of \\(X_2\\). However, uncorrelatedness does NOT imply independence."
  },
  {
    "objectID": "dummy-var.html#fixed-effects",
    "href": "dummy-var.html#fixed-effects",
    "title": "5  Dummy Variables",
    "section": "5.1 Fixed Effects",
    "text": "5.1 Fixed Effects\nOne way of interpreting a model with an (exhaustive) set of dummy variables is as a model with a group-specific constant. Since, it makes no difference to parameter of interest whether you include the constant and \\(J-1\\) dummies or exclude the constant and include all \\(J\\) dummy variables, we can consider the model,1\n\\[\nY_i = \\beta Z_i + \\sum_{j=1}^J \\gamma_j D_{ji} + \\upsilon_i\n\\]\nA common short-hand notation for such a set-up is,\n\\[\nY_i = \\beta Z_i + \\gamma_j + \\upsilon_i\n\\]\nwhere \\(\\gamma_j\\) signifies the presence of \\(J\\) group fixed effects. For each group the constant, denoted by \\(\\gamma\\), changes. However, it is important to remember that this notation is a short-hand. The full set of regressors includes \\(J\\) dummy variables (or a constant and \\(J-1\\) dummy variables). When applying this shorthand it is standard to drop the constant term."
  },
  {
    "objectID": "dummy-var.html#multi-level-fixed-effects",
    "href": "dummy-var.html#multi-level-fixed-effects",
    "title": "5  Dummy Variables",
    "section": "5.2 Multi-level Fixed Effects",
    "text": "5.2 Multi-level Fixed Effects\nSuppose you had two categorical variables, where one was a proper subset of the other. For example, an individual level dataset that contained information on the country in the UK where an individual lived (i.e. England, Scotland, Wales, Northern Ireland) and their county (i.e. Warwickshire, Oxfordshire, Cambridgeshire, etc.). Since county borders do not overlap country borders in the UK, an individuals county perfectly predicts their country.\nConsider then the model,\n\\[\nY_i = \\beta Z_i + \\gamma_j + \\delta_k + \\upsilon_i\n\\]\nwhere \\(j\\) represents country fixed effects and \\(k\\) county fixed effects. Let’s assume the data is sorted by country and then county, and that there are two countries, each with 2 counties. (An obvious simplification.) If we consider the matrix of fixed effects, they can be written as,\n\\[\n  \\begin{bmatrix}1 & 0 & 1 & 0 & 0 & 0 \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  \\vdots & \\vdots & 1 & 0 & 0 & 0 \\\\\n  \\vdots & \\vdots & 0 & 1 & 0 & 0 \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  1 & 0 & 0 & 1 & 0 & 0 \\\\\n  0 & 1 & 0 & 0 & 1 & 0 \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  \\vdots & \\vdots & 0 & 0 & 1 & 0 \\\\\n  \\vdots & \\vdots & 0 & 0 & 0 & 1 \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  0& 1 & 0 & 0 & 0 & 1 \\\\\n  \\end{bmatrix}\n\\]\nWe can immediately see that the matrix is rank deficient. Columns 3 and 4 add up to give you column 1. Likewise, columns 5 and 6 add up to give you column 2. This implies that the matrix is not invertible and we cannot separately identify all fixed effects.\nIf we include column 1 (i.e. dummy variable for country \\(j=1\\)), we must exclude either column 3 or 4 (i.e. one of the counties in country \\(j=1\\)). For the same reasons discussed above (when selecting between the inclusion of the constant and dummy variables), it makes no difference whether we include column 1 and drop either column 3 or 4; or keep columns 3 and 4, and exclude column 1. Thus, we can only identify the model,2\n\\[\nY_i = \\beta Z_i + \\tilde{\\delta}_k + \\upsilon_i\n\\] The general rule is, you retain the “highest” level of fixed effect. In this application, the model with county fixed effects."
  },
  {
    "objectID": "dummy-var.html#fixed-effects-in-panel-data",
    "href": "dummy-var.html#fixed-effects-in-panel-data",
    "title": "5  Dummy Variables",
    "section": "5.3 Fixed Effects in Panel Data",
    "text": "5.3 Fixed Effects in Panel Data\nFixed effects are used extensively in panel data models. They are used to control for time-invariant, unit-level heterogeneity and flexible, aggregate time trends. Consider, the model\n\\[\n  Y_{it} = \\beta Z_{it} + \\alpha_i + \\delta_t + \\varepsilon_{it}\n\\]\nAs before, this notation is used as a shorthand. The full set of unit fixed effects are given by,\n\\[\n\\alpha_i=\\sum_{j=1}^N\\alpha_j\\mathbf{1}\\{i=j\\}\n\\]\nand the time fixed effects are given by,\n\\[\n\\delta_t=\\sum_{j=1}^T\\delta_k\\mathbf{1}\\{t=k\\}\n\\]\nWhat if we wanted to include group-level controls in the model; e.g., an indicator for treatment group status? If group membership is stable over time, then for the reasons discussed above, group membership is perfectly co-linear with the unit fixed effects. The dummy variables for treated units (a subset of the unit fixed effects) add up to the dummy variable for the treated group. You will see this again in EC338.\nA final point on this. What if you need to include a variable that identifies treatment-group status in the model for identification. For example, in a simple two-group-two-period difference-in-differences model you have,\n\\[\n  Y_{it} = \\alpha + \\psi D_i + \\delta T_t + \\beta D_i\\cdot T_t + \\varepsilon_{it}\n\\]\nwhere \\(D_i = \\mathbf{1}\\{treated\\}\\) and \\(T_t = \\mathbf{1}\\{t\\geq t_0\\}\\) where \\(t_0\\) is the period of treatment. The coefficient on the interaction term is the parameter of interest. If we include unit fixed effects in this model,\n\\[\n  Y_{it} = \\alpha_i + \\delta T_t + \\beta D_i\\cdot T_t + \\upsilon_{it}\n\\]\nthen we drop the treatment-group indicator dummy variable: \\(D_i\\). And if we include include time fixed effects,\n\\[\n  Y_{it} = \\alpha_i + \\delta_t + \\beta D_i\\cdot T_t + \\upsilon_{it}\n\\]\nwe drop \\(T_t\\). Is this a problem for identification? No, the group indicator, \\(D_i\\), is “implied” by unit fixed effects. Because the group dummy is perfectly co-linear with the unit fixed effects, you could always have included it by excluding one of the unit fixed effects. In fact, with a balanced panel, replacing the group indicator with unit fixed effects will not actually change the estimated coefficient on the interaction term. It will, however, reduce the standard error of estimator.\nFor this reason, the most common notation used for (static) difference-in-differences is,\n\\[\n  Y_{it} = \\alpha_i + \\delta_t + \\beta D_{it} + \\upsilon_{it}\n\\]\nwhere \\(D_{it} = \\mathbf{1}\\{treated\\}\\cdot\\mathbf{1}\\{t\\geq t_0\\}\\). This notation is particularly useful because it does not need to be adapted when you add more time periods. However, it can only be used in applications with longitudinal data. If you are using repeated cross-sections then you cannot include unit fixed effects and must retain a treatment group indicator. We will discuss this further in EC338."
  },
  {
    "objectID": "dummy-var.html#footnotes",
    "href": "dummy-var.html#footnotes",
    "title": "5  Dummy Variables",
    "section": "",
    "text": "The choice of base category makes a difference when the variable of interest is a categorical variable. In this instance, the choice of base category changes the interpretation of the coefficient. This discussion relates more to instances where the categorical variable is included as a control variable.↩︎\nI deliberately changed the parameters denoting the county fixed effects from \\(\\delta_k\\) to \\(\\tilde{\\delta}_k\\) since the true model (data generating process) may contain both country and county fixed effects. However, we cannot separately identify these. We can identify \\(\\tilde{\\delta}_k\\): a combination of both sets of parameters.↩︎"
  },
  {
    "objectID": "compendium.html",
    "href": "compendium.html",
    "title": "6  Compendium",
    "section": "",
    "text": "7 General case\nSuppose \\(f(x)\\in R\\) (i.e. a scalar) and \\(x\\in R^n\\) (i.e. a \\(n\\times 1\\) vector). Then we can define the partial derivative of \\(f(x)\\) w.r.t. \\(x\\) as,\n\\[\n  \\frac{\\partial f(x)}{\\partial x}  = \\begin{bmatrix}\\frac{\\partial f(x)}{\\partial x_1} \\\\ \\frac{\\partial f(x)}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f(x)}{\\partial x_n} \\end{bmatrix}\n\\]\nA special case is when \\(f(x)\\) is linear in \\(x\\),\n\\[\nf(x) = a'x = \\sum_{i=1}^n a_ix_i\n\\] for \\(a\\in R^n\\). The derivative of \\(a'x\\) with respect to the vector \\(x\\) can be defined as,\n\\[\n  \\begin{aligned}\n  \\frac{\\partial a'x}{\\partial x} & = \\begin{bmatrix}\\frac{\\partial a'x}{\\partial x_1} \\\\ \\frac{\\partial a'x}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial a'x}{\\partial x_n} \\end{bmatrix} \\\\\n  & = \\begin{bmatrix}a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_n \\end{bmatrix} \\\\\n  & = a\n  \\end{aligned}\n\\] since the the partial derivate of \\(a'x = \\sum_{i=1}^n a_ix_i\\) w.r.t. \\(x_i\\) is just the scalar \\(a_i\\).\nThe derivative of a scalar w.r.t. to a vector yields a vector of partial derivatives.\nSince \\(a'x\\) is a scalar, it is by definition symmetric: \\(a'x = x'a\\). Thus,\n\\[\n\\frac{\\partial x'a}{\\partial x} = \\frac{\\partial a'x}{\\partial x} = a\n\\] # Linear: vector case\nSuppose \\(f(x)\\) is a linear transformation of \\(x\\),\n\\[\nf(x) = A'x\n\\] for any \\(m\\times n\\) matrix A,\n\\[\n  A = \\begin{bmatrix}a_1' \\\\ a_2' \\\\ \\vdots \\\\ a_m'\\end{bmatrix}\n\\] where \\(a_i\\in R^n\\;\\forall i=1,\\dots,m\\) and,\n\\[\n  Ax = \\begin{bmatrix}a_1'x \\\\ a_2'x \\\\ \\vdots \\\\ a_m'x\\end{bmatrix}\n\\] Note, \\(f(x)=Ax\\in R^m\\), a \\(m\\times 1\\) vector. We can then define,\n\\[\n  \\begin{aligned}\n  \\frac{\\partial Ax}{\\partial x'} & = \\begin{bmatrix}\\frac{\\partial a_1'x}{\\partial x_1} & \\frac{\\partial a_1'x}{\\partial x_2} & \\dots & \\frac{\\partial a_1'x}{\\partial x_n}\\\\ \\frac{\\partial a_2'x}{\\partial x_1} & \\frac{\\partial a_2'x}{\\partial x_2} & \\dots & \\frac{\\partial a_2'x}{\\partial x_n}\\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ \\frac{\\partial a_m'x}{\\partial x_1} & \\frac{\\partial a_m'x}{\\partial x_2} & \\dots & \\frac{\\partial a_m'x}{\\partial x_n}\\\\ \\end{bmatrix} \\\\\n  & = \\begin{bmatrix} a_{11} & a_{12} & \\dots & a_{1n} \\\\ a_{21} & a_{22} & \\dots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ a_{m1} & a_{m2} & \\dots & a_{mn} \\\\ \\end{bmatrix} \\\\\n  & = A\n  \\end{aligned}\n\\]\nSince Ax is \\(m\\times 1\\) column vector, we take the derivative w.r.t. \\(x'\\) a row vector and not the column vector \\(x\\). This results in a matrix of partial derivatives.\nA second special case is where the function takes on the quadaratic form,\n\\[\nf(x) = x'Ax = \\sum_{i=1}^N\\sum_{j=1}^n a_{ij}x_ix_j\n\\]\nfor \\(n\\times n\\) (square) matrix A. As in the first linear case, \\(f(x)\\) is scalar.\nDefine \\(c = Ax\\), the \\(x'Ax = x'c\\). From the linear case, we know that,\n\\[\n\\frac{\\partial x'c}{\\partial x} = c\n\\]\nSimilarly, if we define \\(d = A'x\\) then \\(x'Ax = d'x\\). From the linear case, we know that,\n\\[\n\\frac{\\partial d'x}{\\partial x} = d\n\\]\nWe can define the total derivative as the sum of the partial derivatives w.r.t. to the first and second \\(x\\). Combining these two results, we have that,\n\\[\n\\frac{\\partial x'Ax}{\\partial x} = Ax + A'x\n\\]\nAnd if \\(A\\) is symmetric, this result simplifies to \\(2Ax\\)."
  },
  {
    "objectID": "compendium.html#linear-independence",
    "href": "compendium.html#linear-independence",
    "title": "6  Compendium",
    "section": "6.1 Linear independence",
    "text": "6.1 Linear independence\nConsider a set of \\(k\\) \\(n\\)-dimensional vectors \\(\\{X_{1},X_{2},...,X_{k}\\}\\). These vectors are,\n\n[definition] linearly dependent if there exists a set of scalars \\(\\{a_{1},a_{2},\\dots,a_{k}\\}\\) such that\n\\[\na_1X_1 + a_2X_2+\\ldots+a_kX_k=0\n\\]\nwhere at least one \\(a_i\\neq0\\).\n\nAlternatively, they are,\n\n[definition] linearly independent if the only set of scalars \\(\\{a_{1},a_{2},\\dots,a_{k}\\}\\) that satisfies the above condition is \\(a_1,a_2,\\dots,a_k=0\\).\n\nIf we collect these \\(k\\) column-vectors in a matrix, \\(X=[X_1\\;X_2 \\dots X_k]\\), then the linear dependence condition can be written as,\n\\[\na_1X_1 + a_2X_2+\\ldots+a_kX_k=\\begin{bmatrix} X_1\\;X_2 \\dots X_k\\end{bmatrix}\\begin{bmatrix}a_1\\\\a_2\\\\\\vdots\\\\a_k\\end{bmatrix}=Xa = 0\n\\]\nGiven any \\(n\\times k\\) matrix \\(X\\), its columns are,\n\n[definition] linearly dependent if there exists a vector \\(a\\in\\mathbb{R}^k\\) such that \\(a\\neq0\\) and \\(Xa=0\\);\n\nor\n\n[definition] linearly independent if the only vector \\(a\\in\\mathbb{R}^k\\) such that \\(Xa=0\\) is \\(a=0\\).\n\nFor any matrix there may be more than one vector \\(a\\in\\mathbb{R}^{k}\\) such that \\(Xa=0\\). Indeed, if both \\(a_{1},a_{2}\\in\\mathbb{R}^{k}\\)\nsatisfy this condition and \\(a_{1}\\neq a_{2}\\) then you can show that any linear combination of \\(\\{a_{1},a_{2}\\}\\) satisfies the\ncondition \\(X(a_{1}b_{1}+a_{2}b_{2})=0\\) for \\(b_{1},b_{2}\\in\\mathbb{R}\\). Thus, there exists an entire set of vectors which satisfy this condition. This set is referred to as the,\n\n[definition] null space of \\(X\\),\n\\[\n\\mathcal{N}(X) = \\{a\\in\\mathbb{R}^k:\\;Xa=0\\}\n\\]\n\nIt should be evident from the definition that if the columns of \\(X\\) are linearly independent then \\(\\mathcal{N}(X)=\\{0\\}\\), a singleton. That is, it just includes the 0-vector."
  },
  {
    "objectID": "compendium.html#vector-spaces-bases-and-spans",
    "href": "compendium.html#vector-spaces-bases-and-spans",
    "title": "6  Compendium",
    "section": "6.2 Vector spaces, bases, and spans",
    "text": "6.2 Vector spaces, bases, and spans\nHere, we concern ourselves only with real vectors from \\(\\mathbb{R}^n\\).\n\n[definition] A vector space, denoted \\(\\mathcal{V}\\), refers to a set of vectors which is closed under finite addition and scalar multiplication.\n\n\n[definition] A set of \\(k\\) linearly independent vectors, \\(\\{X_1,X_2,\\dots,X_k\\}\\), forms a basis for vector space \\(\\mathcal{V}\\) if \\(\\forall\\;y\\in\\mathcal{V}\\) there exists a set of \\(k\\) scalars such that,\n\\[\ny=X_1b_1+X_2b_2+\\ldots+X_kb_k\n\\]\n\nBased on these definitions, it is evident that for the Euclidean space, \\(\\mathbb{E}^n\\), any \\(n\\) linearly independent vectors from \\(\\mathbb{R}^n\\) is a basis. For example, any point in \\(\\mathbb{E}^2\\) can be defined as a multiple of,\n\\[\n\\begin{bmatrix}1\\\\0\\end{bmatrix}\\quad \\text{and} \\quad\\begin{bmatrix}0\\\\1\\end{bmatrix}\n\\]\nConsider again the \\(n\\times k\\) matrix \\(X\\), where \\(k&lt;n\\). Then we define the,\n\n[definition] column space (or span) of \\(X\\), denoted \\(\\mathcal{S}(X)\\), as the vector space generate by the \\(k\\) columns of \\(X\\). Formally,\n\\[\n\\mathcal{S}(X) = \\{y\\in\\mathbb{R}^n:\\;y=Xb\\quad\\text{for some }b\\in \\mathbb{R}^k\\}\n\\]\n\nA property to note about the span or column space \\(X\\) is,\n\n[result] \\(\\mathcal{S}(X)=\\mathcal{S}(XX')\\)\n\nwhere \\(XX'\\) is a \\(n\\times n\\) matrix.\nFinally, we can define the,\n\n[definition] orthogonal column space (or orthogonal span) of \\(X\\) as,\n\\[\n\\mathcal{S}^{\\perp}(X) = \\{y\\in \\mathbb{R}^k:\\;y'x=0\\quad \\forall x\\in\\mathcal{S}(X)\\}\n\\]"
  },
  {
    "objectID": "compendium.html#rank",
    "href": "compendium.html#rank",
    "title": "6  Compendium",
    "section": "6.3 Rank",
    "text": "6.3 Rank\nConsider a \\(n\\times k\\) matrix \\(X\\), the\n\n[definition] row rank of \\(X\\) is the maximum number of linearly independent rows:\n\\[\nrowrank(X) \\leq n\n\\]\n\nWe say that matrix \\(X\\) has full row rank if \\(rowrank(X)=n\\).\nThe,\n\n[definition] column rank of \\(X\\) is the maximum number of linearly independent columns:\n\\[\ncolrank(X) \\leq k\n\\]\n\nWe say that matrix \\(X\\) has full column rank if \\(colrank(X)=k\\).\nAn important result is,\n\n[result] the rank of \\(X\\):\n\\[\nr(X) = rowrank(X)=colrank(X) \\\\\n\\Rightarrow r(X)\\leq min\\{n,k\\}\n\\]\n\nIn addition, since the \\(r(X)\\) depends on the number of linearly independent columns, we can say that,\n\n[result] the dimension of \\(\\mathcal{S}(X)\\), \\(dim(\\mathcal{S}(X))\\), is given by the \\(r(X)\\).\n\nHere are a few additional results,\n\n[result] \\(r(X)=r(X')\\)\n[result] \\(r(XY)\\leq min\\{r(X),r(Y)\\}\\)\n[result] \\(r(XY)=r(X)\\) if \\(Y\\) is square and full rank\n[result] \\(r(X+Y)\\leq r(X) + r(Y)\\)"
  },
  {
    "objectID": "compendium.html#properties-of-square-matrices",
    "href": "compendium.html#properties-of-square-matrices",
    "title": "6  Compendium",
    "section": "6.4 Properties of square matrices",
    "text": "6.4 Properties of square matrices\nConsider the case of a square, \\(n\\times n\\), matrix \\(A\\). We say that,\n\n[definition] \\(A\\) is singular if the \\(r(A)&lt;n\\),\n\nor that,\n\n[definition] \\(A\\) is non-singular if the \\(r(A)=n\\).\n\nThe singularity of a square matrix is important as it determines the invertibility of a matrix, which typically relates the existence of a unique solution in systems of linear equations. Here are a few key results,\n\n[result] There exists a matrix \\(B=A^{-1}\\), such that \\(AB=I_n\\) (where \\(I_n\\) is the identity matrix), if and only if \\(A\\) is non-singular.\n[result] \\(A\\) is non-singular if and only if the determinant of \\(A\\) is non-zero: \\(det(A)\\neq0\\).1\n[result] Likewise, \\(A\\) is singular if and only if \\(det(A)=0\\).\n[result] \\(AA^{-1}=A^{-1}A=I\\)\n[result] \\((A')^{-1}=(A^{-1})'\\)\n[result] If their respective inverses exist, then \\((AB)^{-1}=B^{-1}A^{-1}\\).\n[result] \\(det(AB)=det(A)det(B)\\)\n[result] \\(det(A^{-1})=det(A)^{-1}\\)\n\nFor any square matrix \\(A\\),\n\n[definition] the trace of \\(A\\) is the sum of all diagonal elements:\n\\[\ntr(A) = \\sum_{i=1}^na_{ii}\n\\]\n\nRegarding the trace of a square matrix, here are a few important results:\n\n[result] \\(tr(A+B) = tr(A) + tr(B)\\)\n[result] \\(tr(\\lambda A) = \\lambda tr(A)\\) where \\(\\lambda\\) is a scalar\n[result] \\(tr(A) = tr(A')\\)\n[result] \\(tr(AB) = tr(BA)\\) where \\(AB\\) and \\(BA\\) are both square, but need not be of the same order.\n[result] \\(||A|| = (tr(A'A))^{1/2}\\)"
  },
  {
    "objectID": "compendium.html#properties-of-symmetric-matrices",
    "href": "compendium.html#properties-of-symmetric-matrices",
    "title": "6  Compendium",
    "section": "6.5 Properties of symmetric matrices",
    "text": "6.5 Properties of symmetric matrices\nA symmetric matrix has the property that \\(A=A'\\). Therefore, \\(A\\) must be square.\nHere are a few important results concerning symmetric matrices.\n\n[result] \\(A^{-1}\\) exists if \\(det(A)\\neq 0\\) and \\(r(A)=n\\)\n[result] A is diagonalizable.2\n[result] The eigenvector decomposition of a square matrix gives you \\(A=C\\Lambda C^{-1}\\) where \\(\\Lambda\\) is a diagonal matrix of eigenvalues and $C$ a matrix of the corresponding eigenvectors. The symmetry of \\(A\\) gives you that \\(C^{-1}=C'\\Rightarrow A=C\\Lambda C'\\) with \\(C'C=CC'=I_{n}\\).3\n\nA key definition concerning symmetric matrices is their positive definiteness:\n\n[definition] \\(A\\) is positive semi-definite if for any \\(x\\in\\mathbb{R}^n,\\;x'Ax\\geq0\\).\n\nGiven the eigenvector decomposition of a symmetric matrix, positive semi-definiteness implies \\(\\Lambda\\) is positive semi-definite: \\(\\lambda_i\\geq0\\quad\\forall i\\). Likewise,\n\n[definition] \\(A\\) is positive definite if for any \\(x\\in\\mathbb{R}^n,\\;x'Ax&gt;0\\).\n\nAgain, based on the egeinvector decomposition, positive semi-definiteness implies \\(\\Lambda\\) is positive definite: \\(\\lambda_i&gt;0\\quad\\forall i\\).\nA few more results are:\n\n[result] \\(tr(A) = \\sum_{i=1}^n\\lambda_i\\)\n[result] \\(r(A) = r(\\Lambda)\\)\n[result] \\(det(A) = \\prod_{i=1}^n \\lambda_i\\)\n\nThis last result can be used to prove that any positive definite matrix is non-singular and therefore has an inverse.\nAny full-rank, positive semi-definite, symmetric matrix \\(B\\) has the additional properties:\n\n[result] \\(B=C\\Lambda C'\\) and \\(B^{-1} = C\\Lambda^{-1}C'\\)\n[result] We can define the square-root of \\(B\\) as \\(B^{1/2} = C\\Lambda^{1/2}C'\\). Similarly, \\(B^{-1/2} = C\\Lambda^{-1/2}C'\\)."
  },
  {
    "objectID": "compendium.html#properties-of-idempotent-matrices",
    "href": "compendium.html#properties-of-idempotent-matrices",
    "title": "6  Compendium",
    "section": "6.6 Properties of idempotent matrices",
    "text": "6.6 Properties of idempotent matrices\nAn idempotent matrix has the property that \\(D=DD\\). Therefore, \\(D\\) must be square.\nHere are a few important results concerning idempotent matrices.\n\n[result] \\(D\\) is positive definite\n[result] \\(D\\) is diagonalizable\n[result] \\((I_n-D)\\) is also an idempotent matrix\n[result] With the exception of \\(I_n\\), all idempotent matrices are singular.\n[result] \\(r(D) = tr(D) = \\sum_{i=1}^n\\lambda_i\\)\n[result] \\(\\lambda_i\\in\\{0,1\\}\\quad \\forall\\;i\\)\n\nProjection matrices are idempotent, but need not be symmetric. However, for the purposes of this module we will deal exclusively with symmetric idempotent projection matrices."
  },
  {
    "objectID": "compendium.html#vector-differentiation",
    "href": "compendium.html#vector-differentiation",
    "title": "6  Compendium",
    "section": "6.7 Vector Differentiation",
    "text": "6.7 Vector Differentiation\nHere we will look at the derivatives of scalar with respect to (W.r.t.) a vector. You can also define other derivatives, such as the derivative of a vector w.r.t. a vector and the derivative of a scalar with respect to a matrix. However, these are not needed for these notes."
  },
  {
    "objectID": "compendium.html#footnotes",
    "href": "compendium.html#footnotes",
    "title": "6  Compendium",
    "section": "",
    "text": "These notes do not cover how to calculate the determinant of a square matrix. You should be able to find a definition easily online.↩︎\nA matrix is diagonalizable if it is similar to some other diagonal matrix. Matrices \\(B\\) and \\(C\\) are similar if \\(C=PBP^{-1}\\). A square matrix which is not diagonalizable is defective. This property relates closely to eigenvector decomposition.↩︎\nRecall, an eigenvalue and eigenvector pair, \\((\\lambda,c)\\), of matrix \\(A\\) satisfy:\n\\[\nAc = \\lambda c\\Rightarrow (A-\\lambda I_n)c=0\n\\]↩︎"
  }
]