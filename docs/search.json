[
  {
    "objectID": "geometry-ols.html#partitioned-regression",
    "href": "geometry-ols.html#partitioned-regression",
    "title": "4  The Geometry of OLS",
    "section": "4.1 Partitioned regression",
    "text": "4.1 Partitioned regression\nThe tools of linear algebra can help us better understand partitioned regression. Indeed, I would go as far to to say that it is quite difficult to understand partitioned regression without an understanding of projection matrices. Moreover, we need to understand partitioned regression to really understand multivariate regression.\nLet us divide the set of regressors into two groups: \\(X_1\\) a single regressor and \\(X_2\\) a \\(n\\times (k-1)\\) matrix. We can rewrite the linear model as,\n\\[\nY = X\\beta+ \\varepsilon =\\beta_1X_1+X_2\\beta_2+\\varepsilon\n\\]\nPartitioned regression is typically taught as follows. The OLS estimator for \\(\\beta_1\\) can achieved by first regressing \\(X_1\\) on \\(X_2\\),\n\\[\nX_1 = X_2\\gamma_2+\\upsilon_1\n\\]\nNext, you regress \\(Y\\) on the residual from the above model,\n\\[\nY = \\gamma_1 \\hat{\\upsilon}_1+\\xi\n\\]\nThe partitioned regression result states that the OLS estimator for \\(\\hat{\\gamma_1}=\\hat{\\beta_1}\\). This aids in our understanding of \\(\\beta_1\\) as the partial effect of \\(X_1\\) on \\(Y\\), holding \\(X_2\\) constant.\nWe can show this using projection matrices. Let us begin by applying our existing knowledge. From above, we know that the residual from the regression of \\(X_1\\) on \\(X_2\\) is,\n\\[\n\\hat{\\upsilon} = M_2X_1\n\\]\nwhere \\(M_2 = I_n-X_2(X_2'X_2)^{-1}X_2'\\). Thus, the model we estimate in the second step, is\n\\[\nY = \\gamma_1M_2X_1+\\xi\n\\]\nWe know that \\(\\hat{\\gamma}_1 = (\\hat{\\upsilon}'\\hat{\\upsilon})^{-1}\\hat{\\upsilon}'Y\\). Replacing the value of the residual, we get\n\\[\n\\begin{align}\n\\hat{\\gamma}_1 =& (\\hat{\\upsilon}'\\hat{\\upsilon})^{-1}\\hat{\\upsilon}'Y \\\\\n=&(X_1'M_2M_2X_1)^{-1}X_1'M_2Y \\\\\n=&(X_1'M_2X_1)^{-1}X_1'M_2Y\n\\end{align}\n\\] &gt; [note] We use both the symmetry and idempotent quality of \\(M_2\\).\nNext we want to show that \\(\\hat{\\beta}_1\\) is given by the same value. This part is more complicated. Let’s start with by reminding ourselves of the following:\n\\[\n\\begin{align}\nX'X\\hat{\\beta} &= X'Y \\\\\n\\begin{bmatrix}X_1 & X_2\\end{bmatrix}'\\begin{bmatrix}X_1 & X_2\\end{bmatrix}\\begin{bmatrix}\\hat{\\beta}_1 \\\\ \\hat{\\beta}_2\\end{bmatrix} &= \\begin{bmatrix}X_1 & X_2\\end{bmatrix}'Y \\\\\n\\begin{bmatrix}X_1'X_1 & X_1'X_2 \\\\ X_2'X_1 & X_2'X_2 \\end{bmatrix}\\begin{bmatrix}\\hat{\\beta}_1 \\\\ \\hat{\\beta}_2\\end{bmatrix} &= \\begin{bmatrix}X_1'Y \\\\ X_2'Y\\end{bmatrix}\n\\end{align}\n\\] We could solve for \\(\\hat{\\beta}_1\\) by solving for the inverse of \\(X'X\\); however, this will take a long time. An easier approach is to simply verify that \\(\\hat{\\beta}_1=(X_1'M_2X_1)^{-1}X_1'M_2Y\\). Recall, \\(\\hat{\\beta}\\) splits \\(Y\\) into two components:\n\\[\nY = \\hat{\\beta}_1X_1+X_2\\hat{\\beta}_2 + \\hat{\\varepsilon}\n\\]\nIf we plug this definition of \\(Y\\) into the above expression we get,\n\\[\n\\begin{align}\n&(X_1'M_2X_1)^{-1}X_1'M_2(\\hat{\\beta}_1X_1+X_2\\hat{\\beta}_2 + \\hat{\\varepsilon}) \\\\\n=&\\hat{\\beta}_1\\underbrace{(X_1'M_2X_1)^{-1}X_1'M_2X_1}_{=I_n} \\\\\n&+\\underbrace{(X_1'M_2X_1)^{-1}X_1'M_2X_2\\hat{\\beta_2}}_{=0} \\\\\n&+\\underbrace{(X_1'M_2X_1)^{-1}X_1'M_2\\hat{\\varepsilon}}_{=0} \\\\\n=&\\hat{\\beta}_1\n\\end{align}\n\\] In line 2, I use the fact that \\(\\hat{\\beta}_1\\) is a scalar and can be moved to the front (since the order of multiplication does not matter with a scalar). In line 3, I use the fact that \\(M_2X_2=0\\) by definition. Line 4 uses the fact that \\(M_2\\hat{\\varepsilon}=\\hat{\\varepsilon}\\) which means that \\(X_1'M_2\\hat{\\varepsilon}=X_1'\\hat{\\varepsilon}=0\\).\nThe OLS estimator solves for \\(\\beta_1\\) using the variance in \\(X_1\\) that is orthogonal to \\(X_2\\). This is the manner in which we “hold \\(X_2\\) constant”: the variation in \\(M_2X_1\\) is orthogonal to \\(X_2\\). Changes in \\(M_2X_1\\) are uncorrelated with changes in \\(X_2\\); as if the variation in \\(M_2X_1\\) arose independently of \\(X_2\\). However, uncorrelatedness does NOT imply independence."
  },
  {
    "objectID": "dummy-var.html#fixed-effects",
    "href": "dummy-var.html#fixed-effects",
    "title": "5  Dummy Variables",
    "section": "5.1 Fixed Effects",
    "text": "5.1 Fixed Effects\nOne way of interpreting a model with an (exhaustive) set of dummy variables is as a model with a group-specific constant. Since, it makes no difference to parameter of interest whether you include the constant and \\(J-1\\) dummies or exclude the constant and include all \\(J\\) dummy variables, we can consider the model,1\n\\[\nY_i = \\beta Z_i + \\sum_{j=1}^J \\gamma_j D_{ji} + \\upsilon_i\n\\]\nA common short-hand notation for such a set-up is,\n\\[\nY_i = \\beta Z_i + \\gamma_j + \\upsilon_i\n\\] where \\(\\gamma_j\\) signifies the presence of \\(J\\) group fixed effects. For each group the constant, denoted by \\(\\gamma\\), changes. However, it is important to remember that this notation is a short-hand. The full set of regressors includes \\(J\\) dummy variables (or a constant and \\(J-1\\) dummy variables). When applying this shorthand it is standard to drop the constant term."
  },
  {
    "objectID": "dummy-var.html#multi-level-fixed-effects",
    "href": "dummy-var.html#multi-level-fixed-effects",
    "title": "5  Dummy Variables",
    "section": "5.2 Multi-level Fixed Effects",
    "text": "5.2 Multi-level Fixed Effects\nSuppose you had two categorical variables, where one was a proper subset of the other. For example, an individual level dataset that contained information on the country in the UK where an individual lived (i.e. England, Scotland, Wales, Northern Ireland) and their county (i.e. Warwickshire, Oxfordshire, Cambridgeshire, etc.). Since county borders do not overlap country borders in the UK, an individuals county perfectly predicts their country.\nConsider then the model,\n\\[\nY_i = \\beta Z_i + \\gamma_j + \\delta_k + \\upsilon_i\n\\] where \\(j\\) represents country fixed effects and \\(k\\) county fixed effects. Let’s assume the data is sorted by country and then county, and that there are two countries, each with 2 counties. (An obvious simplification.) If we consider the matrix of fixed effects, they can be written as,\n\\[\n  \\begin{bmatrix}1 & 0 & 1 & 0 & 0 & 0 \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  \\vdots & \\vdots & 1 & 0 & 0 & 0 \\\\\n  \\vdots & \\vdots & 0 & 1 & 0 & 0 \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  1 & 0 & 0 & 1 & 0 & 0 \\\\\n  0 & 1 & 0 & 0 & 1 & 0 \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  \\vdots & \\vdots & 0 & 0 & 1 & 0 \\\\\n  \\vdots & \\vdots & 0 & 0 & 0 & 1 \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  0& 1 & 0 & 0 & 0 & 1 \\\\\n  \\end{bmatrix}\n\\] We can immediately see that the matrix is rank deficient. Columns 3 and 4 add up to give you column 1. Likewise, columns 5 and 6 add up to give you column 2. This implies that the matrix is not invertible and we cannot separately identify all fixed effects.\nIf we include column 1 (i.e. dummy variable for country \\(j=1\\)), we must exclude either column 3 or 4 (i.e. one of the counties in country \\(j=1\\)). For the same reasons discussed above (when selecting between the inclusion of the constant and dummy variables), it makes no difference whether we include column 1 and drop either column 3 or 4; or keep columns 3 and 4, and exclude column 1. Thus, we can only identify the model,2\n\\[\nY_i = \\beta Z_i + \\tilde{\\delta}_k + \\upsilon_i\n\\] The general rule is, you retain the “highest” level of fixed effect. In this application, the model with county fixed effects."
  },
  {
    "objectID": "dummy-var.html#fixed-effects-in-panel-data",
    "href": "dummy-var.html#fixed-effects-in-panel-data",
    "title": "5  Dummy Variables",
    "section": "5.3 Fixed Effects in Panel Data",
    "text": "5.3 Fixed Effects in Panel Data\nFixed effects are used extensively in panel data models. They are used to control for time-invariant, unit-level heterogeneity and flexible, aggregate time trends. Consider, the model\n\\[\n  Y_{it} = \\beta Z_{it} + \\alpha_i + \\delta_t + \\varepsilon_{it}\n\\] As before, this notation is used as a shorthand. The full set of unit fixed effects are given by, \\[\n\\alpha_i=\\sum_{j=1}^N\\alpha_j\\mathbf{1}\\{i=j\\}\n\\] and the time fixed effects are given by, \\[\n\\delta_t=\\sum_{j=1}^T\\delta_k\\mathbf{1}\\{t=k\\}\n\\] What if we wanted to include group-level controls in the model; e.g., an indicator for treatment group status? If group membership is stable over time, then for the reasons discussed above, group membership is perfectly co-linear with the unit fixed effects. The dummy variables for treated units (a subset of the unit fixed effects) add up to the dummy variable for the treated group. You will see this again in EC338.\nA final point on this. What if you need to include a variable that identifies treatment-group status in the model for identification. For example, in a simple two-group-two-period difference-in-differences model you have,\n\\[\n  Y_{it} = \\alpha + \\psi D_i + \\delta T_t + \\beta D_i\\cdot T_t + \\varepsilon_{it}\n\\] where \\(D_i = \\mathbf{1}\\{treated\\}\\) and \\(T_t = \\mathbf{1}\\{t\\geq t_0\\}\\) where \\(t_0\\) is the period of treatment. The coefficient on the interaction term is the parameter of interest. If we include unit fixed effects in this model,\n\\[\n  Y_{it} = \\alpha_i + \\delta T_t + \\beta D_i\\cdot T_t + \\upsilon_{it}\n\\] then we drop the treatment-group indicator dummy variable: \\(D_i\\). And if we include include time fixed effects, \\[\n  Y_{it} = \\alpha_i + \\delta_t + \\beta D_i\\cdot T_t + \\upsilon_{it}\n\\] we drop \\(T_t\\). Is this a problem for identification? No, the group indicator, \\(D_i\\), is “implied” by unit fixed effects. Because the group dummy is perfectly co-linear with the unit fixed effects, you could always have included it by excluding one of the unit fixed effects. In fact, with a balanced panel, replacing the group indicator with unit fixed effects will not actually change the estimated coefficient on the interaction term. It will, however, reduce the standard error of estimator.\nFor this reason, the most common notation used for (static) difference-in-differences is,\n\\[\n  Y_{it} = \\alpha_i + \\delta_t + \\beta D_{it} + \\upsilon_{it}\n\\] where \\(D_{it} = \\mathbf{1}\\{treated\\}\\cdot\\mathbf{1}\\{t\\geq t_0\\}\\). This notation is particularly useful because it does not need to be adapted when you add more time periods. However, it can only be used in applications with longitudinal data. If you are using repeated cross-sections then you cannot include unit fixed effects and must retain a treatment group indicator. We will discuss this further in EC338."
  },
  {
    "objectID": "dummy-var.html#footnotes",
    "href": "dummy-var.html#footnotes",
    "title": "5  Dummy Variables",
    "section": "",
    "text": "The choice of base category makes a difference when the variable of interest is a categorical variable. In this instance, the choice of base category changes the interpretation of the coefficient. This discussion relates more to instances where the categorical variable is included as a control variable.↩︎\nI deliberately changed the parameters denoting the county fixed effects from \\(\\delta_k\\) to \\(\\tilde{\\delta}_k\\) since the true model (data generating process) may contain both country and county fixed effects. However, we cannot separately identify these. We can identify \\(\\tilde{\\delta}_k\\): a combination of both sets of parameters.↩︎"
  }
]