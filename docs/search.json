[
  {
    "objectID": "dummy-var.html",
    "href": "dummy-var.html",
    "title": "5  Dummy Variables",
    "section": "",
    "text": "Dummy variables are used extensively in Microeconometrics. This because of the discrete nature of most policy ‘treatments’ studied in Microeconomics. A dummy variable, \\(D_i=\\{0,1\\}\\), can be used to split the sample into two (or more) groups; e.g., treatment and control.\nIn basic setting the use a of dummy variable might be relatively straight forward. For example, consider a linear model using to assess a single treatment from a randomized control trial,\n\\[\nY_i = \\beta_{10}+\\beta_{11} D_{1i} + \\varepsilon_i\n\\] where \\(D_{1i} = \\mathbf{1}\\{treated\\}\\) identifies those who are treated in the sample.\nFor ease of demonstration, let us assume that the data is sorted on \\(D_i\\): the first \\(N_0\\) observations are the untreated control group, and the next \\(N-N_0\\) the treated. From previous chapters, we know that the matrix of regressors in this model is given by,\n\\[\nX_1=[\\iota,D_1]=\\begin{bmatrix}1 & 0 \\\\ \\vdots & \\vdots \\\\ 1 & 0 \\\\ 1 & 1 \\\\ \\vdots & \\vdots \\\\ 1 & 1 \\end{bmatrix}\n\\] The above matrix has rank 2, as the columns are linearly independent. Likewise, the rank of the \\(2\\times 2\\) matrix \\(X_1'X_1\\) is also 2 (i.e. full rank) and it’s inverse exists. We can see immediately that if \\(N=N_0\\) - that is, no units were treated - the matrix would be rank deficient. Similarly, if \\(N_0=0\\) - that is, all units were treated - the \\(D\\) would be a column of \\(1\\)’s, like the constant.\nConsider the regressor \\(D_2 = 1-D_1 = \\mathbf{1}\\{control\\}\\). What would happen if we included \\(G_i\\) in the above model? The matrix of regressors would be, \\[\nX=[\\iota,D_1,D_2]=\\begin{bmatrix}1 & 0 &1 \\\\ \\vdots & \\vdots & \\vdots \\\\ 1 & 0 &1 \\\\ 1 & 1 &0\\\\ \\vdots & \\vdots & \\vdots \\\\ 1 & 1 & 0 \\end{bmatrix}\n\\] The rank of matrix \\(\\tilde{X}\\) is not 3; it remains 2. The three columns are linearly dependent: \\[\n\\begin{bmatrix}1 \\\\ \\vdots \\\\ 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix}=\\begin{bmatrix}0 \\\\ \\vdots \\\\ 0 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix}+\\begin{bmatrix}1 \\\\ \\vdots \\\\ 1 \\\\ 0\\\\  \\vdots \\\\  0 \\end{bmatrix}\n\\] You can have ANY two of the columns in the model, but not all three. Moreover, regardless which two you choose, the projection remains the same. Consider, the three possible models,\n\\[\n\\begin{align}\nY_i =& \\beta_{10}+\\beta_{11} D_{1i}+\\varepsilon_i=X_1\\beta_1+\\varepsilon_i & \\qquad\\text{where} \\quad X_1 = [\\iota\\;D_1] \\\\\nY_i =& \\beta_{20}+\\beta_{21} D_{2i}+\\varepsilon_i=X_2\\beta_2+\\varepsilon_i & \\qquad\\text{where} \\quad X_1 = [\\iota\\;D_2] \\\\\nY_i =& \\beta_{30}+\\beta_{31} D_{1i}+\\varepsilon_i=X_3\\beta_3+\\varepsilon_i & \\qquad\\text{where} \\quad X_3 = [D_1\\;D_2]\n\\end{align}\n\\] It turns out that, \\[\nP_1 = X_1(X_1'X_1)^{-1}X_1' = P_2 = P_3\n\\]\nSimilarly,\n\\[\nM_1 = I_n-X_1(X_1'X_1)^{-1}X_1' = M_2 = M_3\n\\]\nThus, the vector of residuals estimated by OLS for the above three models are all the same.\nWhy does this matter? Consider, the case where you include a set of dummy variables as additional covariates. For example, when include dummy variables for a categorical variable.\n\\[\nY_i = \\alpha + \\beta Z_i + \\sum_{j=1}^J \\gamma_j D_{ji} + \\upsilon_i\n\\] We know from the previous discussion on partitioned regression that we can estimate the above model"
  },
  {
    "objectID": "ols.html#the-uni-variate-case",
    "href": "ols.html#the-uni-variate-case",
    "title": "3  Ordinary Least Squares",
    "section": "3.1 The uni-variate case",
    "text": "3.1 The uni-variate case\nUndergraduate textbooks all teach a very similar expression for the OLS estimator of a uni-variate regression model (with a constant), such as\n\\[\nY_i = \\beta_1+\\beta_2X_{i2}+\\varepsilon_i\n\\]\n\n[note] Once you are familiar with vector notation, it is relatively easy to tell whether a model is uni- or multi-variate. This is because the notation \\(\\beta_2 X_{i2}\\) is not consistent with \\(X_{2i}\\) being a vector (row or column).\nIf \\(X_{i2}\\) is a \\(k\\times 1\\) vector then so is \\(\\beta_2\\). Thus, \\(\\beta_2 X_{i2}\\) is \\((k\\times 1)\\cdot (k\\times1)\\), which is not defined.\nIf \\(X_{i2}\\) is a row vector (as in Wooldridge, 2011), \\(\\beta_2 X_{i2}\\) will then be \\((k\\times 1)\\cdot (1\\times k)\\), a \\(k\\times k\\) matrix. This cannot be correct since the model is defined at the unit level.\nThus, if you see a model written with the parameter in front of the regressor, you know that this must be a single regressor. This is subtle, yet imporant, distinction that researchers often use to convey the structure of their model. Whenever \\(X_{i2}\\) is a vector, researchers will almost always use the notation \\(X_{i2}'\\beta\\) or \\(X_{i2}\\beta\\), depending on whether \\(X_{i2}\\) is assumed to be a column or row vector.\n\nWe know that,\n\\[\n\\begin{align}\n\\tilde{\\beta}_2 =& \\frac{\\sum(Y_i-\\bar{Y})(X_{i2}-\\bar{X}_2)}{\\sum_{i=1}^n(X_{i2}-\\bar{X}_2)^2} \\\\\n\\text{and}\\qquad \\tilde{\\beta}_1 =& \\bar{Y}-\\tilde{\\beta_2}\\bar{X}_2\n\\end{align}\n\\]\nI am deliberately using the notation \\(\\tilde{\\beta}\\) to distinguish these two estimators from the expression below.\nLet us see if we can replicate this result. When written in vector notation, the model is,\n\\[\n\\begin{align}\nY =& X\\beta+\\varepsilon \\\\\n=& \\begin{bmatrix}1&X_{12} \\\\ 1 & X_{22} \\\\ \\vdots & \\vdots \\\\ 1 & X_{n2}\\end{bmatrix}\\begin{bmatrix}\\beta_1 \\\\ \\beta_2 \\end{bmatrix} + \\varepsilon \\\\\n=& \\begin{bmatrix}\\ell &X_{2} \\end{bmatrix}\\begin{bmatrix}\\beta_1 \\\\ \\beta_2 \\end{bmatrix} + \\varepsilon\n\\end{align}\n\\]\nTherefore,\n\\[\n\\begin{align}\n\\hat{\\beta} = \\begin{bmatrix}\\hat{\\beta}_1 \\\\ \\hat{\\beta}_2\\end{bmatrix}=&(X'X)^{-1}X'Y \\\\\n=&\\bigg(\\begin{bmatrix}\\ell' \\\\ X_{2}' \\end{bmatrix}\\begin{bmatrix}\\ell &X_{2}\\end{bmatrix}\\bigg)^{-1}\\begin{bmatrix}\\ell'  \\\\ X_{2}' \\end{bmatrix}Y \\\\\n=&\\begin{bmatrix}\\ell'\\ell & \\ell'X_2 \\\\ X_{2}'\\ell & X_2'X_2 \\end{bmatrix}^{-1}\\begin{bmatrix}\\ell'Y  \\\\ X_{2}'Y \\end{bmatrix}\n\\end{align}\n\\]\nI went through this rather quickly, using a number of linear algebra rules that you may not be familiar with. Do not worry, the point of the exercise is not become a linear algebra master, but instead to focus on the element of each of each matrix/vector. Each element is a scalar (size \\(1\\times 1\\)).\nIf we right them each down as sums you they might be a little more familiar. (Take a look back at Chapter 1 to remind yourself of some of these steps). First consider the \\(2\\times 2\\) matrix:\n\nelement [1,1]: \\(\\ell'\\ell = \\sum_{i=1}^n 1 = n\\)\nelement [1,2]: \\(\\ell'X_2 = \\sum_{i=1}^nX_{i2} = n\\bar{X}_2\\)\nelement [2,1]: \\(X_2'\\ell = \\sum_{i=1}^nX_{i2} = n\\bar{X}_2\\) (as above, since scalars are symmetric)\nelement [2,2]: \\(X_2'X_2=\\sum_{i=1}^nX_{i2}^2\\)\n\nNext, consider the final \\(2\\times 1\\) vector,\n\nelement [1,1]: \\(\\ell'Y = \\sum_{i=1}^n Y_i = n\\bar{Y}\\)\nelement [2,1]: \\(X_2'Y = \\sum_{i=1}^nY_iX_{i2}\\)\n\nOur OLS estimator is therefore,\n\\[\n\\hat{\\beta} = \\begin{bmatrix} n & n\\bar{X}_2 \\\\ n\\bar{X}_2 & \\sum_{i=1}^nX_{i2}^2 \\end{bmatrix}^{-1}\\begin{bmatrix}n\\bar{Y}_2  \\\\ \\sum_{i=1}^nY_iX_{i2} \\end{bmatrix}\n\\]\nWe now need to solve for the inverse of the \\(2\\times 2\\) matrix. You can easily find notes on how to do this online. Here, I will just provide the solution.\n\\[\n\\hat{\\beta} = \\frac{1}{n\\sum_{i=1}^nX_{i2}^2-n^2\\bar{X}_2^2}\\begin{bmatrix} \\sum_{i=1}^nX_{i2}^2 & -n\\bar{X}_2 \\\\ -n\\bar{X}_2 &  n\\end{bmatrix}\\begin{bmatrix}n\\bar{Y}_2  \\\\ \\sum_{i=1}^nY_iX_{i2} \\end{bmatrix}\n\\]\nRemember, this is still a \\(2\\times 1\\) vector. We can now solve for the final solution:\n\\[\n\\begin{align}\n\\hat{\\beta} =& \\frac{1}{n\\sum_{i=1}^nX_{i2}^2-n^2\\bar{X}_2^2}\\begin{bmatrix} n\\bar{Y}\\sum_{i=1}^nX_{i2}^2 -n\\bar{X}_2\\sum_{i=1}^nY_iX_{i2} \\\\ n\\sum_{i=1}^nY_iX_{i2}-n^2\\bar{X}_2\\bar{Y}\\end{bmatrix} \\\\\n=& \\frac{1}{n\\sum_{i=1}^n(X_{i2}-\\bar{X}_2)^2}\\begin{bmatrix} n\\bar{Y}\\sum_{i=1}^nX_{i2}^2 + n^2\\bar{Y}\\bar{X}^2 - n^2\\bar{Y}\\bar{X}^2 -n\\bar{X}_2\\sum_{i=1}^nY_iX_{i2} \\\\ n\\sum_{i=1}^n(Y_i-\\bar{Y})(X_{i2}-\\bar{X}_2) \\end{bmatrix} \\\\\n=& \\frac{1}{n\\sum_{i=1}^n(X_{i2}-\\bar{X}_2)^2}\\begin{bmatrix} n\\bar{Y}\\sum_{i=1}^n(X_{i2}-\\bar{X})^2 -n\\bar{X}_2\\sum_{i=1}^n(Y_i-\\bar{Y})(X_{i2}-\\bar{X}_2) \\\\ n\\sum_{i=1}^n(Y_i-\\bar{Y})(X_{i2}-\\bar{X}_2) \\end{bmatrix} \\\\\n=& \\begin{bmatrix} \\bar{Y}  -\\frac{n\\sum_{i=1}^n(Y_i-\\bar{Y})(X_{i2}-\\bar{X}_2)}{n\\sum_{i=1}^n(X_{i2}-\\bar{X}_2)^2}\\bar{X}_2 \\\\ \\frac{n\\sum_{i=1}^n(Y_i-\\bar{Y})(X_{i2}-\\bar{X}_2)}{n\\sum_{i=1}^n(X_{i2}-\\bar{X}_2)^2} \\end{bmatrix} \\\\\n=& \\begin{bmatrix} \\bar{Y}  -\\tilde{\\beta}_2\\bar{X}_2 \\\\ \\tilde{\\beta}_2 \\end{bmatrix} \\\\\n=& \\begin{bmatrix}\\tilde{\\beta}_1 \\\\ \\tilde{\\beta}_2 \\end{bmatrix}\n\\end{align}\n\\]\nThe math is a little involved, but it shows you these solutions are are the same. Unfortunately, the working gets even more arduous in a multivariate context. However, there are useful tools to help us with that we will discuss next."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This e-book provides some pre-reading material for the EC338: Econometrics 2 - Microeconometrics module. In line with the Applied Microeconomics and Microeconometrics literatures, EC338 has evolved over time to focus on causal inference, the identification strategies that underpin various casual estimands, and their corresponding estimators.\nAs a result of this shift in focus, less time is given to more advanced material relating to the linear estimators used in this literature. As the relevant material is covered in EC226: Econometrics 1, you should be able to follow EC338 with a little revision and study. However, the change in notation may ‘trip you up’ and certain topics will be easier to understand with a richer understanding of linear estimators. For this reason, I have prepared this material as a brief (re-)introduction to ordinary least squares (OLS).\nIn EC338 we work extensively with dummy variables. As you know from EC226, estimating a linear regression model with discrete independent variables is relatively simple and the interpretation of the coefficient is typically straight-forward. However, the use of dummy variables requires a careful consideration of (perfect) collinearity and to understand collinearity (or rank conditions) it helps to think of data as a matrix, or system of column vectors. This becomes even more important when we start to consider models with various dimensions of fixed-effects.\nThese notes begin by revisiting the basic linear regression model and OLS estimator using vector notation. Next we revisit the properties of OLS, using this same notation, but without extensive proofs. Proofs should be available from a range of textbooks, including Wooldridge (2011). The material that will be least familiar to you will be the geometry of OLS. Here we will cover projection matrices and how they can be used to understand partitioned regression. Finally, we discuss dummy variables, their projections, and issues of colinearity.\nRemember, this is still the same material covered in EC226, just using vector notation. In many instances this simplifies the notation, as summations over \\(n\\) and/or \\(t\\) can be replaced with a simple inner product of vectors. For example, consider the average of random variable \\(Y_i\\),\n\\[\n\\bar{Y} = \\frac{1}{n}\\sum_{i=1}^{n}Y_i\n\\]\nIf we define two \\(n\\times 1\\) column vectors,\n\\[\n\\ell = \\begin{bmatrix}1 \\\\ 1 \\\\ \\vdots \\\\ 1\\end{bmatrix}\\qquad \\text{and}\\qquad Y = \\begin{bmatrix}Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\\end{bmatrix}\n\\]\nthen the inner produce of these vectors is,\n\\[\n\\langle\\ell,Y\\rangle = \\ell'Y = 1\\cdot Y_1+1\\cdot Y_2+\\dots+1\\cdot Y_n = \\sum_{i=1}^{n}Y_i\n\\]\nIn addition,\n\\[\n\\langle\\ell,\\ell\\rangle = \\ell'\\ell = 1\\cdot 1+1\\cdot 1+\\dots+1\\cdot 1 = \\sum_{i=1}^{n}1 = n\n\\]\nThus, the average can be expressed as,\n\\[\n(\\ell'\\ell)^{-1}\\ell'Y = \\frac{1}{n}\\sum_{i=1}^{n}Y_i\n\\]\nIt turns out, this linear transformation of \\(Y\\),\n\\[\nAY = (\\ell'\\ell)^{-1}\\ell'Y\n\\]\nhas a structure that is common to many linear estimators; a structure that derives from the projection matrix of the \\(\\ell\\)-vector: \\(P_{\\ell} =\\ell(\\ell'\\ell)^{-1}\\ell'\\) . We will discuss this in Chapter Chapter 4 when we explore the geometry of OLS.\nAt the end of this e-book is a compendium on linear algebra basics. I may reference these during the e-book. The compendium is not exhaustive and does not include proofs. Please consult a linear algebra text book for further reading if you require.\nThe material in this e-book will not be examined directly, but will assist with your understanding of the material covered during term."
  },
  {
    "objectID": "dummy-var.html#fixed-effects",
    "href": "dummy-var.html#fixed-effects",
    "title": "5  Dummy Variables",
    "section": "5.1 Fixed Effects",
    "text": "5.1 Fixed Effects\nOne way of interpreting a model with an (exhaustive) set of dummy variables is as a model with a group-specific constant. Since, it makes no difference to parameter of interest whether you include the constant and \\(J-1\\) dummies or exclude the constant and include all \\(J\\) dummy variables, we can consider the model,1\n\\[\nY_i = \\beta Z_i + \\sum_{j=1}^J \\gamma_j D_{ji} + \\upsilon_i\n\\]\nA common short-hand notation for such a set-up is,\n\\[\nY_i = \\beta Z_i + \\gamma_j + \\upsilon_i\n\\] where \\(\\gamma_j\\) signifies the presence of \\(J\\) group fixed effects. For each group the constant, denoted by \\(\\gamma\\), changes. However, it is important to remember that this notation is a short-hand. The full set of regressors includes \\(J\\) dummy variables (or a constant and \\(J-1\\) dummy variables). When applying this shorthand it is standard to drop the constant term."
  },
  {
    "objectID": "dummy-var.html#fixed-effects-in-panel-data",
    "href": "dummy-var.html#fixed-effects-in-panel-data",
    "title": "5  Dummy Variables",
    "section": "5.3 Fixed Effects in Panel Data",
    "text": "5.3 Fixed Effects in Panel Data\nFixed effects are used extensively in panel data models. They are used to control for time-invariant, unit-level heterogeneity and flexible, aggregate time trends. Consider, the model\n\\[\n  Y_{it} = \\beta Z_{it} + \\alpha_i + \\delta_t + \\varepsilon_{it}\n\\] As before, this notation is used as a shorthand. The full set of unit fixed effects are given by, \\[\n\\alpha_i=\\sum_{j=1}^N\\alpha_j\\mathbf{1}\\{i=j\\}\n\\] and the time fixed effects are given by, \\[\n\\delta_t=\\sum_{j=1}^T\\delta_k\\mathbf{1}\\{t=k\\}\n\\] What if we wanted to include group-level controls in the model; e.g., an indicator for treatment group status? If group membership is stable over time, then for the reasons discussed above, group membership is perfectly co-linear with the unit fixed effects. The dummy variables for treated units (a subset of the unit fixed effects) add up to the dummy variable for the treated group. You will see this again in EC338.\nA final point on this. What if you need to include a variable that identifies treatment-group status in the model for identification. For example, in a simple two-group-two-period difference-in-differences model you have,\n\\[\n  Y_{it} = \\alpha + \\psi D_i + \\delta T_t + \\beta D_i\\cdot T_t + \\varepsilon_{it}\n\\] where \\(D_i = \\mathbf{1}\\{treated\\}\\) and \\(T_t = \\mathbf{1}\\{t\\geq t_0\\}\\) where \\(t_0\\) is the period of treatment. The coefficient on the interaction term is the parameter of interest. If we include unit fixed effects in this model,\n\\[\n  Y_{it} = \\alpha_i + \\delta T_t + \\beta D_i\\cdot T_t + \\upsilon_{it}\n\\] then we drop the treatment-group indicator dummy variable: \\(D_i\\). And if we include include time fixed effects, \\[\n  Y_{it} = \\alpha_i + \\delta_t + \\beta D_i\\cdot T_t + \\upsilon_{it}\n\\] we drop \\(T_t\\). Is this a problem for identification? No, the group indicator, \\(D_i\\), is “implied” by unit fixed effects. Because the group dummy is perfectly co-linear with the unit fixed effects, you could always have included it by excluding one of the unit fixed effects. In fact, with a balanced panel, replacing the group indicator with unit fixed effects will not actually change the estimated coefficient on the interaction term. It will, however, reduce the standard error of estimator.\nFor this reason, the most common notation used for (static) difference-in-differences is,\n\\[\n  Y_{it} = \\alpha_i + \\delta_t + \\beta D_{it} + \\upsilon_{it}\n\\] where \\(D_{it} = \\mathbf{1}\\{treated\\}\\cdot\\mathbf{1}\\{t\\geq t_0\\}\\). This notation is particularly useful because it does not need to be adapted when you add more time periods. However, it can only be used in applications with longitudinal data. If you are using repeated cross-sections then you cannot include unit fixed effects and must retain a treatment group indicator. We will discuss this futher in EC338."
  },
  {
    "objectID": "dummy-var.html#footnotes",
    "href": "dummy-var.html#footnotes",
    "title": "5  Dummy Variables",
    "section": "",
    "text": "The choice of base category makes a difference when the variable of interest is a categorical variable. In this instance, the choice of base category changes the interpretation of the coefficient. This discussion relates more to instances where the categorical variable is included as a control variable.↩︎\nI deliberately changed the parameters denoting the county fixed effects from \\(\\delta_k\\) to \\(\\tilde{\\delta}_k\\) since the true model (data generating process) may contain both country and county fixed effects. However, we cannot separately identify these. We can identify \\(\\tilde{\\delta}_k\\): a combination of both sets of parameters.↩︎"
  },
  {
    "objectID": "dummy-var.html#multi-level-fixed-effects",
    "href": "dummy-var.html#multi-level-fixed-effects",
    "title": "5  Dummy Variables",
    "section": "5.2 Multi-level Fixed Effects",
    "text": "5.2 Multi-level Fixed Effects\nSuppose you had two categorical variables, where one was a proper subset of the other. For example, an individual level dataset that contained information on the country in the UK where an individual lived (i.e. England, Scotland, Wales, Northern Ireland) and their county (i.e. Warwickshire, Oxfordshire, Cambridgeshire, etc.). Since county borders do not overlap country borders in the UK, an individuals county perfectly predicts their country.\nConsider then the model,\n\\[\nY_i = \\beta Z_i + \\gamma_j + \\delta_k + \\upsilon_i\n\\] where \\(j\\) represents country fixed effects and \\(k\\) county fixed effects. Let’s assume the data is sorted by country and then county, and that there are two countries, each with 2 counties. (An obvious simplification.) If we consider the matrix of fixed effects, they can be written as,\n\\[\n  \\begin{bmatrix}1 & 0 & 1 & 0 & 0 & 0 \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  \\vdots & \\vdots & 1 & 0 & 0 & 0 \\\\\n  \\vdots & \\vdots & 0 & 1 & 0 & 0 \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  1 & 0 & 0 & 1 & 0 & 0 \\\\\n  0 & 1 & 0 & 0 & 1 & 0 \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  \\vdots & \\vdots & 0 & 0 & 1 & 0 \\\\\n  \\vdots & \\vdots & 0 & 0 & 0 & 1 \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  0& 1 & 0 & 0 & 0 & 1 \\\\\n  \\end{bmatrix}\n\\] We can immediately see that the matrix is rank deficient. Columns 3 and 4 add up to give you column 1. Likewise, columns 5 and 6 add up to give you column 2. This implies that the matrix is not invertible and we cannot separately identify all fixed effects.\nIf we include column 1 (i.e. dummy variable for country \\(j=1\\)), we must exclude either column 3 or 4 (i.e. one of the counties in country \\(j=1\\)). For the same reasons discussed above (when selecting between the inclusion of the constant and dummy variables), it makes no difference whether we include column 1 and drop either column 3 or 4; or keep columns 3 and 4, and exclude column 1. Thus, we can only identify the model,2\n\\[\nY_i = \\beta Z_i + \\tilde{\\delta}_k + \\upsilon_i\n\\] The general rule is, you retain the “highest” level of fixed effect. In this application, the model with county fixed effects."
  },
  {
    "objectID": "linear-reg.html#vector-notation",
    "href": "linear-reg.html#vector-notation",
    "title": "2  The Linear Regression Model",
    "section": "2.1 Vector notation",
    "text": "2.1 Vector notation\nMost undergraduate textbooks discuss data in terms of random variables: the dependent or outcome variable (typically denoted \\(Y_i\\)) and various independent or explanatory variables (\\(X_{i1},X_{i2},\\ldots,X_{ik}\\)). There’s nothing wrong with this language, but to understand the geometry of OLS we will need to think in terms of random vectors.\nWhen working with cross-sectional data, we think of a random sample as a collection of \\(n\\) realizations of the same random variable. Each observation represents a different unit (e.g., individual, firm, classroom, etc.) and we typically add the assumption that the data is i.i.d. (independently and identically distributed) across units of observation. It makes no difference then if we think of this random sample as a collection of \\(n\\times 1\\) random vectors, where each row represents a different unit and the unit of observation is maintained across random vector.\nFor each unit you observe the outcome (\\(Y_i\\)) and a vector of explanatory variables (or regressors):\n\\[\nX_i = \\begin{bmatrix}X_{i1} \\\\ X_{i2} \\\\ \\vdots \\\\ X_{ik}\\end{bmatrix}\n\\]\nTake note of the ordering of the subscripts: the first denotes the unit of observation (\\(i\\)) and the second the number of the regressor (\\(j=1\\dots k\\)). The pair \\((Y_i,X_i)\\) represents an observation, where \\(Y_i\\) is a single random variable and \\(X_i\\) a random (column) vector.1 A collection of observations forms a sample.\nYou are, no doubt, familiar with the linear regression model. A simple univariate model is typically written as,\n\\[\nY_i = \\beta_1 + \\beta_2 X_{i2} + \\varepsilon_i\n\\]\nWhere \\(\\beta_1\\) is the constant (intercept) and \\(\\beta_2\\) the slope coefficient. In EC338, we will discuss in more detail the justification for this model. For now, let us focus on notation.\nThe linear regression model is linear in parameters, which means we can express the outcome as a linear transformation of a finite set of parameters (i.e. \\(k\\times 1\\) vector of parameters). These (population) parameters are assumed to be constants and unknown to the econometrician.\nWe can rewrite the above equation using vectors,\n\\[\n\\begin{align} Y_i &= \\begin{bmatrix}1\\;X_{i2}\\end{bmatrix}\\begin{bmatrix}\\beta_1 \\\\ \\beta_2\\end{bmatrix} + \\varepsilon_i \\\\\n&= \\begin{bmatrix}1\\\\ X_{i2}\\end{bmatrix}'\\begin{bmatrix}\\beta_1 \\\\ \\beta_2\\end{bmatrix} + \\varepsilon_i \\\\\n&=X_i'\\beta + \\varepsilon_i\n\\end{align}\n\\]\nWhere \\(X_i\\) is a column vector including the number 1 in the first row (for the constant/intercept) and \\(X_{i2}\\) in the second row.\n\n[important] You may find my notation slightly unusual. A lot undergraduate textbooks use different letters (\\(\\alpha\\), \\(\\beta\\), \\(\\gamma\\), etc.) to denote the constant and slope coefficients. As we are collecting all coefficients in a single vector, it helps to use indexes instead of different letters.\nThen there is the issue of the where to start the index: \\(0\\) or \\(1\\). This decision is somewhat arbitrary and hinges on whether the the \\(k\\) regressors included in the model includes the constant term. At the top of the page I described an observation as an outcome and a random vector of regressors. As the constant is not random it is natural to think of it apart from the set of regressors included in the model. However, this decision is somewhat arbitrary. You could simply set \\(X_{i1} = 1\\;\\forall\\;i\\) and the constant would be included in \\(k\\).\nIn EC226, you indexed from 0. Recall, the linear model as \\(k+1\\) parameters; the \\(+1\\) for the constant. When computing the degrees of freedom for the RSS, you solved for \\(n-k-1\\): \\(k\\) regressors plus the constant.\nHere we will index from \\(1\\). This distinction makes it easier to keep track of the size of the matrix. It is also a more natural notation if you consider that the model need not have a constant. The choice of including a constant is therefore no different to including any other regressor. Moreover, when we consider models with fixed effects the constant typically drops from the model.\nThe key thing to remember is that you need to keep track of the number of parameters in the model, that includes the constant if there is one.\n\nWe can easily extend this notation to the case of multivariate regression. For example, consider a model with a constant and \\(k-1\\) regressors.\n\\[\n\\begin{align} Y_i &= \\beta_1 + \\beta_2 X_{i2} + \\beta_3 X_{i3} + \\ldots + \\beta_k X_{ik} + \\varepsilon_i \\\\\n&= \\begin{bmatrix} 1\\; X_{i2}\\; X_{i3}\\; \\dots\\; X_{ik} \\end{bmatrix} \\begin{bmatrix} \\beta_1\\\\ \\beta_2\\\\ \\beta_3\\\\ \\vdots\\\\ \\beta_k\\end{bmatrix} + \\varepsilon_i \\\\\n&=X_i'\\beta + \\varepsilon_i\n\\end{align}\n\\]\nRegardless of the number of regressors, the notation remains the same. Take note of the fact that the \\(X_i\\) is a column vector which means that the notation must include a transpose: \\(X_i'\\beta\\). Excluding the transpose is incorrect since you cannot multiple two \\(k\\times1\\) column vectors. You can multiply a \\(1\\times k\\) row vector with a \\(k\\times 1\\) column vector giving you a \\(1\\times 1\\) scalar. The result should be a scalar since \\(Y_i\\) is a scalar.\nThis notation is not universal. For example, Wooldridge (2011) treats \\(X_i\\) as a row vector. For this reason, the linear regression model can be expressed as \\(X_i\\beta\\). Both notations are used in the applied literature, but I am more familiar with and prefer the column-vector notation."
  },
  {
    "objectID": "linear-reg.html#matrix-notation",
    "href": "linear-reg.html#matrix-notation",
    "title": "2  The Linear Regression Model",
    "section": "2.2 Matrix notation",
    "text": "2.2 Matrix notation\nThe above expressions for the linear regression model all describe a single unit of observation from the sample. Consider, each line included the subscript \\(i\\). Since the model is assumed to be the same for each observation, this an accurate depiction of the linear regression model. However, we also need to think about the correct notation for the entire sample. To do so, we will have to worker with both vectors and matrices.\nSince the model is the same for each observation in the sample, we could imagine “stacking” all \\(n\\) observations on top of one another to form a vector. Consider first the outcome variable,\n\\[\nY=\\begin{bmatrix}Y_1\\\\ Y_2 \\\\ \\vdots\\\\ Y_n \\end{bmatrix}\n\\]\n$$\n$$\n\\(Y\\) is a \\(n\\times 1\\) column vector of all outcomes in the sample. You can distinguish the vector \\(Y\\) from the scalar \\(Y_i\\) by the absence of a subscript.\nSimilarly, we can stack the right-hand side of the equation.\n\\[\n\\begin{align}\nY&=\\begin{bmatrix}X_1'\\beta\\\\ X_2'\\beta \\\\ \\vdots\\\\ X_n'\\beta \\end{bmatrix}+ \\begin{bmatrix}\\varepsilon_1\\\\ \\varepsilon_2 \\\\ \\vdots\\\\ \\varepsilon_n \\end{bmatrix}\\\\\n&=\\begin{bmatrix}X_1'\\\\ X_2' \\\\ \\vdots\\\\ X_n' \\end{bmatrix}\\beta+ \\begin{bmatrix}\\varepsilon_1\\\\ \\varepsilon_2 \\\\ \\vdots\\\\ \\varepsilon_n \\end{bmatrix}\\\\\n&=X\\beta+\\varepsilon\n\\end{align}\n\\]\nLike \\(Y\\), \\(\\varepsilon\\) is a \\(n\\times 1\\) vector. \\(X\\) is a \\(n\\times k\\) matrix and \\(\\beta\\) remains a \\(k\\times 1\\) vector of parameters. The product of a \\(n\\times k\\) matrix and \\(k\\times 1\\) vector is a \\(n\\times 1\\) vector: the same size vector as \\(Y\\) and \\(\\varepsilon\\). As it is important to understand the structure of \\(X\\), let us write it out in detail.\n\\[\nX = \\begin{bmatrix}X_1'\\\\ X_2' \\\\ \\vdots\\\\ X_n' \\end{bmatrix} = \\begin{bmatrix}X_{11} & X_{12}&\\dots&X_{1k}\\\\ X_{21}& X_{22}& & \\\\ \\vdots & & \\ddots &\\\\ X_{n1} & & & X_{nk} \\end{bmatrix}\n\\] The \\(X\\) matrix has \\(n\\) rows, each representing a different unit of observation, and \\(k\\) columns, each representing a different regressor. Recall, one of these regressors may be a constant. If the model includes a constant then \\(X_{i1}=1\\;\\forall\\;i\\). This means that the first column of \\(X\\) is a vector of \\(1\\)’s. Each subsequent column represents another regressor.\nIf you are familiar with rectangular datasets from with working in STATA or R, then you may have notices that \\(X\\) is essentially the “dataset” (excluding the outcome variable). In a rectangular dataset, each row represents a different observation and each column a different variable. That’s what we have here.\nWhy is this noteworthy? When assert that there must be an absence of perfect colinearity between the variables in the model, we are actually saying that the columns of \\(X\\) must be linearly independent. The formal way of expressing this is that \\(X\\) must have full column rank; or \\(r(X)=k\\) (see Chapter 6 for a definition of linear dependence and rank). This is why the OLS condition included in EC226 as “no perfect colinearity” is sometimes referred to as the rank condition. Without full rank, we cannot estimate the linear regression model."
  },
  {
    "objectID": "linear-reg.html#footnotes",
    "href": "linear-reg.html#footnotes",
    "title": "2  The Linear Regression Model",
    "section": "",
    "text": "In these notes, as in the remainder of EC338, I treat \\(X_i\\) as a column vector. Some texts, including Wooldridge (2011), will treat \\(X_i\\) as a row vector. This distinction is not significant, but will affect your notation. I will point this out at a later stage.↩︎"
  },
  {
    "objectID": "geometry-ols.html#partitioned-regression",
    "href": "geometry-ols.html#partitioned-regression",
    "title": "4  The Geometry of OLS",
    "section": "4.1 Partitioned regression",
    "text": "4.1 Partitioned regression\nThe tools of linear algebra can help us better understand partitioned regression. Indeed, I would go as far to to say that it is quite difficult to understand partitioned regression without an understanding of projection matrices. Moreover, we need to understand partitioned regression to really understand multivariate regression.\nLet us divide the set of regressors into two groups: \\(X_1\\) a single regressor and \\(X_2\\) a \\(n\\times (k-1)\\) matrix. We can rewrite the linear model as,\n\\[\nY = X\\beta+ \\varepsilon =\\beta_1X_1+X_2\\beta_2+\\varepsilon\n\\]\nPartitioned regression is typically taught as follows. The OLS estimator for \\(\\beta_1\\) can achieved by first regressing \\(X_1\\) on \\(X_2\\),\n\\[\nX_1 = X_2\\gamma_2+\\upsilon_1\n\\]\nNext, you regress \\(Y\\) on the residual from the above model,\n\\[\nY = \\gamma_1 \\hat{\\upsilon}_1+\\xi\n\\]\nThe partitioned regression result states that the OLS estimator for \\(\\hat{\\gamma_1}=\\hat{\\beta_1}\\). This aids in our understanding of \\(\\beta_1\\) as the partial effect of \\(X_1\\) on \\(Y\\), holding \\(X_2\\) constant.\nWe can show this using projection matrices. Let us begin by applying our existing knowledge. From above, we know that the residual from the regression of \\(X_1\\) on \\(X_2\\) is,\n\\[\n\\hat{\\upsilon} = M_2X_1\n\\]\nwhere \\(M_2 = I_n-X_2(X_2'X_2)^{-1}X_2'\\). Thus, the model we estimate in the second step, is\n\\[\nY = \\gamma_1M_2X_1+\\xi\n\\]\nWe know that \\(\\hat{\\gamma}_1 = (\\hat{\\upsilon}'\\hat{\\upsilon})^{-1}\\hat{\\upsilon}'Y\\). Replacing the value of the residual, we get\n\\[\n\\begin{align}\n\\hat{\\gamma}_1 =& (\\hat{\\upsilon}'\\hat{\\upsilon})^{-1}\\hat{\\upsilon}'Y \\\\\n=&(X_1'M_2M_2X_1)^{-1}X_1'M_2Y \\\\\n=&(X_1'M_2X_1)^{-1}X_1'M_2Y\n\\end{align}\n\\] &gt; [note] We use both the symmetry and idempotent quality of \\(M_2\\).\nNext we want to show that \\(\\hat{\\beta}_1\\) is given by the same value. This part is more complicated. Let’s start with by reminding ourselves of the following:\n\\[\n\\begin{align}\nX'X\\hat{\\beta} &= X'Y \\\\\n\\begin{bmatrix}X_1 & X_2\\end{bmatrix}'\\begin{bmatrix}X_1 & X_2\\end{bmatrix}\\begin{bmatrix}\\hat{\\beta}_1 \\\\ \\hat{\\beta}_2\\end{bmatrix} &= \\begin{bmatrix}X_1 & X_2\\end{bmatrix}'Y \\\\\n\\begin{bmatrix}X_1'X_1 & X_1'X_2 \\\\ X_2'X_1 & X_2'X_2 \\end{bmatrix}\\begin{bmatrix}\\hat{\\beta}_1 \\\\ \\hat{\\beta}_2\\end{bmatrix} &= \\begin{bmatrix}X_1'Y \\\\ X_2'Y\\end{bmatrix}\n\\end{align}\n\\] We could solve for \\(\\hat{\\beta}_1\\) by solving for the inverse of \\(X'X\\); however, this will take a long time. An easier approach is to simply verify that \\(\\hat{\\beta}_1=(X_1'M_2X_1)^{-1}X_1'M_2Y\\). Recall, \\(\\hat{\\beta}\\) splits \\(Y\\) into two components:\n\\[\nY = \\hat{\\beta}_1X_1+X_2\\hat{\\beta}_2 + \\hat{\\varepsilon}\n\\]\nIf we plug this definition of \\(Y\\) into the above expression we get,\n\\[\n\\begin{align}\n&(X_1'M_2X_1)^{-1}X_1'M_2(\\hat{\\beta}_1X_1+X_2\\hat{\\beta}_2 + \\hat{\\varepsilon}) \\\\\n=&\\hat{\\beta}_1\\underbrace{(X_1'M_2X_1)^{-1}X_1'M_2X_1}_{=I_n} \\\\\n&+\\underbrace{(X_1'M_2X_1)^{-1}X_1'M_2X_2\\hat{\\beta_2}}_{=0} \\\\\n&+\\underbrace{(X_1'M_2X_1)^{-1}X_1'M_2\\hat{\\varepsilon}}_{=0} \\\\\n=&\\hat{\\beta}_1\n\\end{align}\n\\] In line 2, I use the fact that \\(\\hat{\\beta}_1\\) is a scalar and can be moved to the front (since the order of multiplication does not matter with a scalar). In line 3, I use the fact that \\(M_2X_2=0\\) by definition. Line 4 uses the fact that \\(M_2\\hat{\\varepsilon}=\\hat{\\varepsilon}\\) which means that \\(X_1'M_2\\hat{\\varepsilon}=X_1'\\hat{\\varepsilon}=0\\).\nThe OLS estimator solves for \\(\\beta_1\\) using the variance in \\(X_1\\) that is orthogonal to \\(X_2\\). This is the manner in which we “hold \\(X_2\\) constant”: the variation in \\(M_2X_1\\) is orthogonal to \\(X_2\\). Changes in \\(M_2X_1\\) are uncorrelated with changes in \\(X_2\\); as if the variation in \\(M_2X_1\\) arose independently of \\(X_2\\). However, uncorrelatedness does NOT imply independence."
  }
]