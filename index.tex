% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrreprt}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\KOMAoption{captions}{tableheading}
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={EC338 Pre-reading},
  pdfauthor={Neil Lloyd},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{EC338 Pre-reading}
\author{Neil Lloyd}
\date{2023-09-18}

\begin{document}
\maketitle
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[breakable, frame hidden, sharp corners, enhanced, boxrule=0pt, borderline west={3pt}{0pt}{shadecolor}, interior hidden]}{\end{tcolorbox}}\fi

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\bookmarksetup{startatroot}

\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}
\addcontentsline{toc}{chapter}{Preface}

\markboth{Preface}{Preface}

This is a Quarto book.

To learn more about Quarto books visit
\url{https://quarto.org/docs/books}.

\bookmarksetup{startatroot}

\hypertarget{sec-intro}{%
\chapter{Introduction}\label{sec-intro}}

This e-book provides some pre-reading material for the \emph{EC338:
Econometrics 2 - Microeconometrics} module. In line with the Applied
Microeconomics and Microeconometrics literatures, EC338 has evolved over
time to focus on causal inference, the identification strategies that
underpin various casual estimands, and their corresponding estimators.

As a result of this shift in focus, less time is given to more advanced
material relating to the linear estimators used in this literature. As
the relevant material is covered in \emph{EC226: Econometrics 1}, you
should be able to follow EC338 with a little revision and study.
However, the change in notation may `trip you up' and certain topics
will be easier to understand with a richer understanding of linear
estimators. For this reason, I have prepared this material as a brief
(re-)introduction to ordinary least squares (OLS).

In EC338 we work extensively with dummy variables. As you know from
EC226, estimating a linear regression model with discrete independent
variables is relatively simple and the interpretation of the coefficient
is \emph{typically} straight-forward. However, the use of dummy
variables requires a careful consideration of (perfect) collinearity and
to understand collinearity (or rank conditions) it helps to think of
data as a matrix, or system of column vectors. This becomes even more
important when we start to consider models with various dimensions of
fixed-effects.

These notes begin by revisiting the basic linear regression model and
OLS estimator using vector notation. Next we revisit the properties of
OLS, using this same notation, but without extensive proofs. Proofs
should be available from a range of textbooks, including Wooldridge
(2011). The material that will be least familiar to you will be the
geometry of OLS. Here we will cover projection matrices and how they can
be used to understand partitioned regression. Finally, we discuss dummy
variables, their projections, and issues of colinearity.

Remember, this is still the same material covered in EC226, just using
vector notation. In many instances this simplifies the notation, as
summations over \(n\) and/or \(t\) can be replaced with a simple inner
product of vectors. For example, consider the average of random variable
\(Y\),

\[
\bar{Y} = \frac{1}{n}\sum_{i=1}^{n}Y_i
\]

If we define the following two n-dimension column vectors,

\[
\iota = \begin{bmatrix}1 \\ 1 \\ \vdots \\ 1\end{bmatrix}\qquad \text{and}\qquad Y = \begin{bmatrix}Y_1 \\ Y_2 \\ \vdots \\ Y_n\end{bmatrix}
\]

then the inner produce of these vectors is,

\[
\iota'Y = 1\cdot Y_1+1\cdot Y_2+\dots+1\cdot Y_n = \sum_{i=1}^{n}Y_i
\]

In addition,

\[
\iota'\iota = 1\cdot 1+1\cdot 1+\dots+1\cdot 1 = \sum_{i=1}^{n}1 = n
\]

Thus, the average can be expressed as,

\[
(\iota'\iota)^{-1}\iota'Y = \frac{1}{n}\sum_{i=1}^{n}Y_i
\]

It turns out, this linear transformation of \(Y\),

\[
AY = (\iota'\iota)^{-1}\iota'Y
\]

has a structure that is common to many linear estimators; a structure
that derives from the projection matrix of the \(\iota\)-vector:
\(P_{\iota} =\iota(\iota'\iota)^{-1}\iota'\) . We will discuss this in
Chapter Chapter~\ref{sec-geometry} when we explore the geometry of OLS.

At the end of this e-book is a compendium on linear algebra basics. I
may reference these during the e-book. The compendium is not exhaustive
and does not include proofs. Please consult a linear algebra text book
for further reading if you require.

The material in this e-book will not be examined directly, but will
assist with your understanding of the material covered during term.

\bookmarksetup{startatroot}

\hypertarget{the-linear-regression-model}{%
\chapter{The Linear Regression
Model}\label{the-linear-regression-model}}

\hypertarget{vector-notation}{%
\section{Vector notation}\label{vector-notation}}

Most undergraduate textbooks discuss data in terms of random variables:
the dependent (outcome) variable (\(Y\)) and various independent
(explanatory) variables (\(X_1,X_2,\ldots,X_k\)). There's nothing wrong
with this language, but to understand the geometry of OLS we will need
to think in random vectors.

Think about the random data as a collection of observations for unit
\(i=1,\ldots,n\). For each unit you observe the outcome (\(Y_i\)) and a
vector of explanatory variables (or regressors):

\[
X_i = \begin{bmatrix}X_{i1} \\ X_{i2} \\ \vdots \\ X_{ik}\end{bmatrix}
\]

Take note of the ordering of the subscripts: the first denotes the unit
of observation (\(i\)) and the second the number of the regressor
(\(k\)). The pair \((Y_i,X_i)\) reflects an \emph{observation}, where
\(Y_i\) is a single random variable and \(X_i\) a random (column)
vector.\footnote{In these notes, as in the remainder of EC338, I treat
  \(X_i\) as a column vector. Some texts, including Wooldridge (2011),
  will treat \(X_i\) as a row vector. This distinction is not
  significant, but will affect your notation. I will point this out at a
  later stage.} A collection of observations forms a sample.

You are, no doubt, familiar with the linear regression model. A simple
univariate model is typically written as,

\[
Y_i = \beta_1 + \beta_2 X_{i2} + \varepsilon_i
\]

Where \(\beta_1\) is the constant (intercept) and \(\beta_2\) the slope
coefficient. In EC338, we will discuss in more detail the justification
for the model. For now, let us focus on notation.

This linear regression model is linear \emph{in parameters}, which means
we can write as a linear function of a set of \(\beta\)-parameters.
These parameters are constant and unknown. Here is how we could rewrite
the above equation using vectors,

\[
\begin{align} Y_i &= \begin{bmatrix}1\;X_{i2}\end{bmatrix}\begin{bmatrix}\beta_1 \\ \beta_2\end{bmatrix} + \varepsilon_i \\
&= \begin{bmatrix}1\\ X_{i2}\end{bmatrix}'\begin{bmatrix}\beta_1 \\ \beta_2\end{bmatrix} + \varepsilon_i \\
&=X_i'\beta + \varepsilon_i
\end{align}
\]

Where \(X_i\) is a column vector including the number 1 in the first row
(for the constant/intercept) and \(X_{i2}\) in the second row.

\begin{quote}
\textbf{{[}important{]}} You find my notation slightly unusual. First of
all, a lot undergraduate textbooks use different letters (\(\alpha\),
\(\beta\), \(\gamma\), etc.) to denote the constant and slope
coefficient(s). Since we want to collect all of the coefficients into a
single vector it makes sense to use indexes instead of different
letters.

Then there is the issue of the where to start the index: \(0\) or \(1\).
This decision is somewhat arbitrary and hinges on whether the the \(k\)
regressors included in the model includes the constant term. At the top
of the page I described an observation as an outcome and a random vector
of regressors. As the constant is not a \emph{random} variable it is
natural to think of it apart from the set of regressors included in the
model. However, this decision is somewhat arbitrary. You could simply
set \(X_{i1} = 1\;\forall\;i\) and the constant would be included in
\(k\).

\textbf{In EC226, you indexed from 0.} Recall, the linear model as
\(k+1\) parameters; the \(+1\) for the constant. When computing the
degrees of freedom for the RSS, you solved for \(n-k-1\): \(k\)
regressors plus the constant.

\textbf{Here we will index from} \(1\)\textbf{.} This distinction makes
it easier to keep track of the size of the matrix. It is also a more
natural notation if you consider that the model need not have a
constant. The choice of including a constant is therefore no different
to including any other regressor. Moreover, when we consider models with
fixed effects the constant typically drops from the model.

The key thing to remember is that you need to keep track of the number
of parameters in the model, that includes the constant \emph{if there is
one}.
\end{quote}

We can easily extend this notation to the case of multivariate
regression. For example, consider a model with a constant and \(k-1\)
regressors.

\[
\begin{align} Y_i &= \beta_1 + \beta_2 X_{i2} + \beta_3 X_{i3} + \ldots + \beta_k X_{ik} + \varepsilon_i \\
&= \begin{bmatrix} 1\; X_{i2}\; X_{i3}\; \dots\; X_{ik} \end{bmatrix} \begin{bmatrix} \beta_1\\ \beta_2\\ \beta_3\\ \vdots\\ \beta_k\end{bmatrix} + \varepsilon_i \\
&=X_i'\beta + \varepsilon_i
\end{align}
\]

Regardless of the number of regressors, the notation remains the same.
Take note of the fact that the \(X_i\) is a column vector which means
that the notation must include a transpose: \(X_i'\beta\). Excluding the
transpose is incorrect since you cannot multiple two \(k\times1\) column
vectors. You can multiply a \(1\times k\) row vector with a
\(k\times 1\) column vector giving you a \(1\times 1\) scalar. The
result should be a scalar since \(Y_i\) is a scalar.

This notation is not universal. For example, Wooldridge (2011) treats
\(X_i\) as a row vector. For this reason, the linear regression model
can be expressed as \(X_i\beta\). Both notations are used in the applied
literature, but I am more familiar with and prefer the column-vector
notation.

\hypertarget{matrix-notation}{%
\section{Matrix notation}\label{matrix-notation}}

The above expressions for the linear regression model all describe a
single unit of observation from the sample. Consider, each line included
the subscript \(i\). Since the model is assumed to be the same for each
observation, this an accurate depiction of the linear regression model.
However, we also need to think about the correct notation for the entire
sample. To do so, we will have to worker with both vectors and matrices.

Since the model is the same for each observation in the sample, we could
imagine ``stacking'' all \(n\) observations on top of one another to
form a vector. Consider first the outcome variable,

\[
Y=\begin{bmatrix}Y_1\\ Y_2 \\ \vdots\\ Y_n \end{bmatrix}
\]

\$\$

\$\$

\(Y\) is a \(n\times 1\) column vector of all outcomes in the sample.
You can distinguish the vector \(Y\) from the scalar \(Y_i\) by the
absence of a subscript.

Similarly, we can stack the right-hand side of the equation.

\[
\begin{align}
Y&=\begin{bmatrix}X_1'\beta\\ X_2'\beta \\ \vdots\\ X_n'\beta \end{bmatrix}+ \begin{bmatrix}\varepsilon_1\\ \varepsilon_2 \\ \vdots\\ \varepsilon_n \end{bmatrix}\\
&=\begin{bmatrix}X_1'\\ X_2' \\ \vdots\\ X_n' \end{bmatrix}\beta+ \begin{bmatrix}\varepsilon_1\\ \varepsilon_2 \\ \vdots\\ \varepsilon_n \end{bmatrix}\\
&=X\beta+\varepsilon
\end{align}
\]

Like \(Y\), \(\varepsilon\) is a \(n\times 1\) vector. \(X\) is a
\(n\times k\) matrix and \(\beta\) remains a \(k\times 1\) vector of
parameters. The product of a \(n\times k\) matrix and \(k\times 1\)
vector is a \(n\times 1\) vector: the same size vector as \(Y\) and
\(\varepsilon\). As it is important to understand the structure of
\(X\), let us write it out in detail.

\[
X = \begin{bmatrix}X_1'\\ X_2' \\ \vdots\\ X_n' \end{bmatrix} = \begin{bmatrix}X_{11} & X_{12}&\dots&X_{1k}\\ X_{21}& X_{22}& & \\ \vdots & & \ddots &\\ X_{n1} & & & X_{nk} \end{bmatrix}
\] The \(X\) matrix has \(n\) rows, each representing a different unit
of observation, and \(k\) columns, each representing a different
regressor. Recall, one of these regressors \emph{may} be a constant. If
the model includes a constant then \(X_{i1}=1\;\forall\;i\). This means
that the first column of \(X\) is a vector of \(1\)'s. Each subsequent
column represents another regressor.

If you are familiar with rectangular datasets from with working in STATA
or R, then you may have notices that \(X\) is essentially the
``dataset'' (excluding the outcome variable). In a rectangular dataset,
each row represents a different observation and each column a different
variable. That's what we have here.

Why is this noteworthy? When assert that there must be an absence of
perfect colinearity between the variables in the model, we are actually
saying that the columns of \(X\) must be linearly independent. The
formal way of expressing this is that \(X\) must have \emph{full} column
rank; or \(r(X)=k\) (see Chapter~\ref{sec-compendium} for a definition
of linear dependence and rank). This is why the OLS condition included
in EC226 as ``no perfect colinearity'' is sometimes referred to as the
rank condition. Without full rank, we cannot estimate the linear
regression model.

\bookmarksetup{startatroot}

\hypertarget{ordinary-least-squares}{%
\chapter{Ordinary Least Squares}\label{ordinary-least-squares}}

You are, no doubt, familiar with the ordinary least squares (OLS)
estimator from your previous studies in Econometrics (EC226 or
otherwise). OLS is \emph{an} estimator for \(\beta\), it is not the only
one. Indeed, you could use maximum likelihood methods to estimate
\(\beta\).

The OLS estimator is the solution to,

\[ \max_b\;\sum_{i=1}^n(Y_i-X_i'b)^2 \]

Using vector notation, we can rewrite this as

\[ \begin{align} &\max_b\;(Y-Xb)'(Y-Xb)\\ =&\max_b\;Y'Y-2b'X'Y+b'X'Xb \end{align} \]

\begin{quote}
\textbf{{[}note{]}} When working with vectors and matrices it is
important to keep track of their size. You can only multiply two
matrices/vectors if their column and row sizes match. For example, if
\(A\) and \(B\) are both \(n\times k\) matrices (\(n\neq k\)), then
\(AB\) is not defined since \(A\) has \(k\) columns and \(B\) \(n\)
rows. For the same reason \(BA\) is also not defined. However, you can
pre-multiply \(B\) with \(A'\) as \(A'\) is a \(k\times n\) matrix:
\(A'B\) is therefore a \(k\times k\) matrix. Similarly, \(B'A\) is
defined, but is a \(n\times n\) matrix.

As you can see, order matters when working with matrices and vectors.
Pre-multiplication and post-multiplication are not the same thing.
\end{quote}

Keep track of the size of each term to ensure they correspond to one
another. In this instance, each term should be a scalar. For example,
\(-2b'X'Y\) is the multiplication of a scalar (\(-2\): size
\(1\times 1\)), row vector (\(b'\): size \(1\times k\)), matrix (\(X'\):
size \(k\times n\)), and column vector (\(Y\): size \(n\times 1\)). Thus
we have a
\((1\times 1)\cdot (1\times k)\cdot (k\times n)\cdot (n\times 1)=1\times 1\).

Differentiating the above expression w.r.t. \(b\) and setting the
first-order conditions to \(0\), we can solve for \(\hat{\beta}\)

\[ \begin{align} &0=-2X'Y+2X'X\hat{\beta} \\ \Rightarrow& X'X\hat{\beta} = X'Y \end{align} \]
What are the dimensions of this equation? Well, \(X'X\) is a
\(k\times k\) matrix and \(\hat{\beta}\) a \(k\times 1\) vector. Thus
the left-hand side of the equation is a \(k\times 1\) vector, as is the
right-hand side. {[}Check that you agree.{]}

In order to solve for \(\hat{\beta}\) we need to move the \(X'X\) term
to the right-hand side. If these were scalars we would simply divide
both sides by the same constant. However, as \(X'X\) is a matrix,
division is not possible. Instead, we need to pre-multiply both sides by
the inverse of \(X'X\): \((X'X)^{-1}\). Here's the issue: the inverse of
a matrix need not exist.

Given a square \(k\times k\) matrix \(A\), its inverse exists \emph{if
and only if} \(A\) is non-singular. For \(A\) to be non-singular its
rank must be equal to \(k\): \(r(A)=k\). This means that all \(k\)
columns/rows must be linearly independent. (See
Chapter~\ref{sec-compendium} for a more detailed discussion of all these
terms.)

In our application, \(A=X'X\) and

\[ r(X'X) = r(X) = colrank(X) \]

To insure that the inverse exists, \(X\) must have full column rank: all
column vectors must be \emph{linearly independent}. In practice, this
means that no regressor can be a \emph{perfect} linear combination of
others. This gives rise to one of the key linear regression model
assumptions:

\begin{quote}
\textbf{{[}assumption{]}} \emph{rank condition:} \(r(X)=k\)
\end{quote}

You may know this assumption by another name: the absense of perfect
colinearity between regressors.

\begin{quote}
\textbf{{[}comment{]}} The rank condition is the reason we exclude a
base category when working with categorical variables. We will revisit
this subject in more detail in Chapter~\ref{sec-dummy}.

Recall, most linear regression models are specified with constant. Thus,
the first column of \(X\) is

\[ X_1 = \begin{bmatrix}1 \\ 1 \\ \vdots \\ 1\end{bmatrix} \] a
\(n\times 1\) vector vector of \(1\)'s, sometimes referred to as
\(\iota\) (the Greek letter \emph{iota}). Suppose you have a categorical
- for example, gender in an individual level dataset - that splits the
same in two. The categories are assumed to be exhaustive and mutually
exclusive. If you create two dummy variables, one for each category,

\[ X_2 = \begin{bmatrix}1 \\ \vdots \\1\\0\\ \vdots \\ 0\end{bmatrix}\qquad\text{and}\qquad X_3 = \begin{bmatrix}0 \\ \vdots \\0\\1\\ \vdots \\ 1\end{bmatrix} \]

it is evident that \(X_2+X_3 = \iota\). (Here I have depicted the sample
as sorted along these two categories.) If \(X=[X_1\;X_2\;X_3]\), then it
is rank-deficient: \(r(X) = 2<3\), since \(X_3=X_1-X2\). Thus, we can
only include two of these three regressors. We can even exclude the
constant and have \(X=[X_2\;X_3]\). In Chapter~\ref{sec-geometry} we
will show that the projection remains the same regardless of which
category we exclude, even the constant.
\end{quote}

If \(X\) is full rank, then \((X'X)^{-1}\) exists and,

\[ \hat{\beta} = (X'X)^{-1}X'Y \]

This relatively simple expression is the solution to least squares
maximization problem, written in matrix notation. Just think, it would
take three lines of code to programme this. That is the power of knowing
a little linear algebra.

ADD CODE

\begin{quote}
\textbf{{[}comment{]}} You may recognise the above expression from the
Chapter~\ref{sec-intro} where we used vectors to define the mean
estimator. It turns out, the mean estimator is the simplest of OLS
estimators. It is a reression of \(Y\) against a constant alone:
\(X=\iota\).
\end{quote}

We can write the same expression in terms of summations over unit-level
observations,

\[ \hat{\beta} = \big(\sum_{i=1}^nX_iX_i')^{-1}\sum_{i=1}^nX_iY_i \]

Note, the change in position of the transpose: \(X_i\) is a column
vector \(\Rightarrow\) \(X_i'X_i\) is a scalar and \(X_iX_i'\) is a
\(k\times k\) matrix. To match the first expression, the term inside the
parenthesis must be a \(k\times k\) matrix. Similarly, \(X'Y\) is a
\(k\times 1\) vector, as is \(X_iY_i\).

\hypertarget{the-uni-variate-case}{%
\section{The uni-variate case}\label{the-uni-variate-case}}

Undergraduate textbooks all teach a very similar expression for the OLS
estimator of a uni-variate regression model (with a constant), such as

\[
Y_i = \beta_1+\beta_2X_{i2}+\varepsilon_i
\]

\begin{quote}
\textbf{{[}note{]}} Once you are familiar with vector notation, it is
relatively easy to tell whether a model is uni- or multi-variate. This
is because the notation \(\beta_2 X_{i2}\) is not consistent with
\(X_{2i}\) being a vector (row or column).

If \(X_{i2}\) is a \(k\times 1\) vector then so is \(\beta_2\). Thus,
\(\beta_2 X_{i2}\) is \((k\times 1)\cdot (k\times1)\), which is not
defined.

If \(X_{i2}\) is a row vector (as in Wooldridge, 2011),
\(\beta_2 X_{i2}\) will then be \((k\times 1)\cdot (1\times k)\), a
\(k\times k\) matrix. This cannot be correct since the model is defined
at the unit level.

Thus, if you see a model written with the parameter in front of the
regressor, you know that this must be a single regressor. This is
subtle, yet imporant, distinction that researchers often use to convey
the structure of their model. Whenever \(X_{i2}\) is a vector,
researchers will \emph{almost always} use the notation \(X_{i2}'\beta\)
or \(X_{i2}\beta\), depending on whether \(X_{i2}\) is assumed to be a
column or row vector.
\end{quote}

We know that,

\[
\begin{align}
\tilde{\beta}_2 =& \frac{\sum(Y_i-\bar{Y})(X_{i2}-\bar{X}_2)}{\sum_{i=1}^n(X_{i2}-\bar{X}_2)^2} \\
\text{and}\qquad \tilde{\beta}_1 =& \bar{Y}-\tilde{\beta_2}\bar{X}_2
\end{align}
\]

I am deliberately using the notation \(\tilde{\beta}\) to distinguish
these two estimators from the expression below.

Let us see if we can replicate this result. When written in vector
notation, the model is,

\[
\begin{align}
Y =& X\beta+\varepsilon \\
=& \begin{bmatrix}1&X_{12} \\ 1 & X_{22} \\ \vdots & \vdots \\ 1 & X_{n2}\end{bmatrix}\begin{bmatrix}\beta_1 \\ \beta_2 \end{bmatrix} + \varepsilon \\
=& \begin{bmatrix}\iota &X_{2} \end{bmatrix}\begin{bmatrix}\beta_1 \\ \beta_2 \end{bmatrix} + \varepsilon
\end{align}
\]

Therefore,

\[
\begin{align} 
\hat{\beta} = \begin{bmatrix}\hat{\beta}_1 \\ \hat{\beta}_2\end{bmatrix}=&(X'X)^{-1}X'Y \\
=&\bigg(\begin{bmatrix}\iota' \\ X_{2}' \end{bmatrix}\begin{bmatrix}\iota &X_{2}\end{bmatrix}\bigg)^{-1}\begin{bmatrix}\iota'  \\ X_{2}' \end{bmatrix}Y \\
=&\begin{bmatrix}\iota'\iota & \iota'X_2 \\ X_{2}'\iota & X_2'X_2 \end{bmatrix}^{-1}\begin{bmatrix}\iota'Y  \\ X_{2}'Y \end{bmatrix}
\end{align} 
\]

I went through this rather quickly, using a number of linear algebra
rules that you may not be familiar with. Do not worry, the point of the
exercise is not become a linear algebra master, but instead to focus on
the element of each of each matrix/vector. Each element is a scalar
(size \(1\times 1\)).

If we right them each down as sums you they might be a little more
familiar. (Take a look back at Chapter~\ref{sec-intro} to remind
yourself of some of these steps). First consider the \(2\times 2\)
matrix:

\begin{itemize}
\item
  element {[}1,1{]}: \(\iota'\iota = \sum_{i=1}^n 1 = n\)
\item
  element {[}1,2{]}: \(\iota'X_2 = \sum_{i=1}^nX_{i2} = n\bar{X}_2\)
\item
  element {[}2,1{]}: \(X_2'\iota = \sum_{i=1}^nX_{i2} = n\bar{X}_2\) (as
  above, since scalars are symmetric)
\item
  element {[}2,2{]}: \(X_2'X_2=\sum_{i=1}^nX_{i2}^2\)
\end{itemize}

Next, consider the final \(2\times 1\) vector,

\begin{itemize}
\item
  element {[}1,1{]}: \(\iota'Y = \sum_{i=1}^n Y_i = n\bar{Y}\)
\item
  element {[}2,1{]}: \(X_2'Y = \sum_{i=1}^nY_iX_{i2}\)
\end{itemize}

Our OLS estimator is therefore,

\[
\hat{\beta} = \begin{bmatrix} n & n\bar{X}_2 \\ n\bar{X}_2 & \sum_{i=1}^nX_{i2}^2 \end{bmatrix}^{-1}\begin{bmatrix}n\bar{Y}_2  \\ \sum_{i=1}^nY_iX_{i2} \end{bmatrix}
\]

We now need to solve for the inverse of the \(2\times 2\) matrix. You
can easily find notes on how to do this online. Here, I will just
provide the solution.

\[
\hat{\beta} = \frac{1}{n\sum_{i=1}^nX_{i2}^2-n^2\bar{X}_2^2}\begin{bmatrix} \sum_{i=1}^nX_{i2}^2 & -n\bar{X}_2 \\ -n\bar{X}_2 &  n\end{bmatrix}\begin{bmatrix}n\bar{Y}_2  \\ \sum_{i=1}^nY_iX_{i2} \end{bmatrix}
\]

Remember, this is still a \(2\times 1\) vector. We can now solve for the
final solution:

\[
\begin{align}
\hat{\beta} =& \frac{1}{n\sum_{i=1}^nX_{i2}^2-n^2\bar{X}_2^2}\begin{bmatrix} n\bar{Y}\sum_{i=1}^nX_{i2}^2 -n\bar{X}_2\sum_{i=1}^nY_iX_{i2} \\ n\sum_{i=1}^nY_iX_{i2}-n^2\bar{X}_2\bar{Y}\end{bmatrix} \\
=& \frac{1}{n\sum_{i=1}^n(X_{i2}-\bar{X}_2)^2}\begin{bmatrix} n\bar{Y}\sum_{i=1}^nX_{i2}^2 + n^2\bar{Y}\bar{X}^2 - n^2\bar{Y}\bar{X}^2 -n\bar{X}_2\sum_{i=1}^nY_iX_{i2} \\ n\sum_{i=1}^n(Y_i-\bar{Y})(X_{i2}-\bar{X}_2) \end{bmatrix} \\
=& \frac{1}{n\sum_{i=1}^n(X_{i2}-\bar{X}_2)^2}\begin{bmatrix} n\bar{Y}\sum_{i=1}^n(X_{i2}-\bar{X})^2 -n\bar{X}_2\sum_{i=1}^n(Y_i-\bar{Y})(X_{i2}-\bar{X}_2) \\ n\sum_{i=1}^n(Y_i-\bar{Y})(X_{i2}-\bar{X}_2) \end{bmatrix} \\
=& \begin{bmatrix} \bar{Y}  -\frac{n\sum_{i=1}^n(Y_i-\bar{Y})(X_{i2}-\bar{X}_2)}{n\sum_{i=1}^n(X_{i2}-\bar{X}_2)^2}\bar{X}_2 \\ \frac{n\sum_{i=1}^n(Y_i-\bar{Y})(X_{i2}-\bar{X}_2)}{n\sum_{i=1}^n(X_{i2}-\bar{X}_2)^2} \end{bmatrix} \\
=& \begin{bmatrix} \bar{Y}  -\tilde{\beta}_2\bar{X}_2 \\ \tilde{\beta}_2 \end{bmatrix} \\
=& \begin{bmatrix}\tilde{\beta}_1 \\ \tilde{\beta}_2 \end{bmatrix}
\end{align}
\]

The math is a little involved, but it shows you these solutions are are
the same. Unfortunately, the working gets even more arduous in a
multivariate context. However, there are useful tools to help us with
that we will discuss next.

\bookmarksetup{startatroot}

\hypertarget{sec-geometry}{%
\chapter{The Geometry of OLS}\label{sec-geometry}}

In the last section we saw how the OLS estimator can, more generally, be
described as a linear transformation of the \(Y\) vector.

\[
\hat{\beta} = (X'X)^{-1}X'Y
\]

We also saw that in order for there to be a (unique) solution to the
least squared problem, the \(X\) matrix must be full rank. This rules
out any perfect colinearity between variables in the \(X\) matrix,
including the constant.

Given the vector of OLS coefficients, we can also estimate the residual,

\[
\begin{align}
\hat{\varepsilon} =& Y - X\hat{\beta} \\
=&Y-X(X'X)^{-1}X'Y \\
=&(I_n-X(X'X)X^{-1})Y
\end{align}
\]

by plugging the definition of \(\hat{\beta}\). Thus, the OLS estimator
separates the vector \(Y\) into two components:

\[
\begin{align}
 Y =& X\hat{\beta} + \hat{\varepsilon}\\
=&\underbrace{X(X'X)^{-1}X'}_{P_X}Y + (\underbrace{I_n-X(X'X)X^{-1}}_{I_n-P_X = M_X})Y \\
=&P_XY + M_XY
\end{align}
\]

The matrix \(P_X = X(X'X)^{-1}X'\) is a \(n\times n\) \emph{projection}
matrix. It is a linear transformation that projects any vector into the
span of \(X\): \(S(X)\). (See Chapter~\ref{sec-compendium} more
information on these terms.) \(S(X)\) is the vector space spanned by the
columns of \(X\). The dimensions of this vector space depends on the
rank of \(P_X\),

\[
dim(S(X)) = r(P_X) = r(X) = k
\]

The matrix \(M_X = I_n-X(X'X)^{-1}X'\) is also a \(n\times n\)
projection matrix. It projects any vector into \(X\)'s \emph{orthogonal}
span: \(S^{\perp}(X)\). Any vector \(z\in S^{\perp}(X)\) is orthogonal
to \(X\). This includes the estimated residual, which is by definition
orthogonal to the predicted values and, indeed, any column of \(X\)
(i.e.~any regressor). The dimension of this orthogonal vector space
depends on the rank of \(M_X\),

\[
dim(S^{\perp}(X)) = r(M_X) = r(I_n)-r(X) = n-k
\]

The orthogonality of these two projections can be easily shown, since
projection matrices are idempotent (\(P_XP_X = P_X\)) and symmetric
(\(P_X' = P_X\)). Consider the inner product of these two projections,

\[
P_X'M_X = P_X(I_n-P_X) = P_X-P_XP_X = P_X-P_X = 0
\]

The least squares estimator is a projection of Y into two vector spaces:
one the span of the columns of \(X\) and the other a space orthogonal to
\(X\).

Why is this useful? Well, it helps us understand the ``mechanics''
(technically geometry) of OLS. When work with linear regression models,
we typically assume either strict exogeneity - \(E[\varepsilon|X]=0\) -
or uncorrelatedness - \(E[X'\varepsilon]=0\) - where the former implies
the latter (but not the other way around).

When we use OLS, we estimate the vector \(\hat{\beta}\) such that,

\[
X'\hat{\varepsilon}=0 \quad always
\]

This is true, \emph{not just in expectation}, but by definition. The
relationship is ``mechanical'': the covariates and estimated residual
are perfectly uncorrelated. This can be easily shown:

\[
\begin{align}
X'\hat{\varepsilon} =& X'M_XY \\
=& X'(I_n-P_X)Y \\
=&X'I_nY-X'X(X'X)^{-1}X'Y \\
=&X'Y-X'Y \\
=&0
\end{align}
\]

You are essentially imposing the assumption of uncorrelatedness between
the explained and unexplained components of Y on the data. This means
that if the assumption is wrong, so is the projection.

\hypertarget{partitioned-regression}{%
\section{Partitioned regression}\label{partitioned-regression}}

The tools of linear algebra also help us to understand partitioned
regression. In fact, I would go as far to to say that it is quite
difficult to understand partitioned regression without an understanding
of projection matrices. Moreover, we need to understand partitioned
regression to really understand multivariate regression.

Let us divide the set of regressors into two groups: \(X_1\) a single
regressor and \(X_2\) a \(n\times (k-1)\) matrix. We can rewrite the
linear model as,

\[
Y = X\beta+ \varepsilon =\beta_1X_1+X_2\beta_2+\varepsilon
\]

Partitioned regression is typically taught as follows. The OLS estimator
for \(\beta_1\) can achieved by first regressing \(X_1\) on \(X_2\),

\[
X_1 = X_2\gamma_2+\upsilon_1
\]

Next, you regress \(Y\) on the residual from the above model,

\[
Y = \gamma_1 \hat{\upsilon}_1+\xi
\]

The OLS estimator for \(\gamma_1\) is equal to that of \(\beta_1\).

Let us begin by applying our existing knowledge. From above, we know
that the residual from the regression of \(X_1\) on \(X_2\) is,

\[
\hat{\upsilon} = M_2X_1
\]

where \(M_2 = I_n-X_2(X_2'X_2)^{-1}X_2'\). Thus, the model we estimate
in the second step, is

\[
Y = \gamma_1M_2X_1+\xi
\]

We know that
\(\hat{\gamma}_1 = (\hat{\upsilon}'\hat{\upsilon})^{-1}\hat{\upsilon}'Y\).
Replacing the value of the residual, we get

\[
\begin{align}
\hat{\gamma}_1 =& (\hat{\upsilon}'\hat{\upsilon})^{-1}\hat{\upsilon}'Y \\
=&(X_1'M_2M_2X_1)^{-1}X_1'M_2Y \\
=&(X_1'M_2X_1)^{-1}X_1'M_2Y
\end{align}
\] \textgreater{} {[}note{]} We use both the symmetry and idempotent
quality of \(M_2\).

Next we want to show that \(\hat{\beta}_1\) is given by the same value.
This part is more complicated. Let's start with by reminding ourselves
of the following:

\[
\begin{align}
X'X\hat{\beta} &= X'Y \\
\begin{bmatrix}X_1 & X_2\end{bmatrix}'\begin{bmatrix}X_1 & X_2\end{bmatrix}\begin{bmatrix}\hat{\beta}_1 \\ \hat{\beta}_2\end{bmatrix} &= \begin{bmatrix}X_1 & X_2\end{bmatrix}'Y \\
\begin{bmatrix}X_1'X_1 & X_1'X_2 \\ X_2'X_1 & X_2'X_2 \end{bmatrix}\begin{bmatrix}\hat{\beta}_1 \\ \hat{\beta}_2\end{bmatrix} &= \begin{bmatrix}X_1'Y \\ X_2'Y\end{bmatrix}
\end{align}
\] We could solve for \(\hat{\beta}_1\) by solving for the inverse of
\(X'X\); however, this will take a long time. An easier approach is to
simply verify that \(\hat{\beta}_1=(X_1'M_2X_1)^{-1}X_1'M_2Y\). Recall,
\(\hat{\beta}\) splits \(Y\) into two components:

\[
Y = \hat{\beta}_1X_1+X_2\hat{\beta}_2 + \hat{\varepsilon}
\]

If we plug this definition of \(Y\) into the above expression we get,

\[
\begin{align}
&(X_1'M_2X_1)^{-1}X_1'M_2(\hat{\beta}_1X_1+X_2\hat{\beta}_2 + \hat{\varepsilon}) \\
=&\hat{\beta}_1\underbrace{(X_1'M_2X_1)^{-1}X_1'M_2X_1}_{=I_n} \\
&+\underbrace{(X_1'M_2X_1)^{-1}X_1'M_2X_2\hat{\beta_2}}_{=0} \\
&+\underbrace{(X_1'M_2X_1)^{-1}X_1'M_2\hat{\varepsilon}}_{=0} \\
=&\hat{\beta}_1
\end{align}
\] In line 2 I use the fact that \(\hat{\beta}_1\) is a scalar and can
be moved to the front (order of multiplication does not matter). In line
3 we exploit the fact that \(M_2X_2=0\) by definition. Line 4 uses the
fact that \(M_2\hat{\varepsilon}=\hat{\varepsilon}\) which means that
\(X_1'M_2\hat{\varepsilon}=X_1'\hat{\varepsilon}=0\).

\bookmarksetup{startatroot}

\hypertarget{sec-dummy}{%
\chapter{Dummy Variables}\label{sec-dummy}}

In summary, this book has no content whatsoever.

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1} \SpecialCharTok{+} \DecValTok{1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 2
\end{verbatim}

\bookmarksetup{startatroot}

\hypertarget{sec-compendium}{%
\chapter{Compendium}\label{sec-compendium}}

\hypertarget{linear-independence}{%
\section{Linear independence}\label{linear-independence}}

Consider a set of \(k\) \(n\)-dimensional vectors
\(\{X_{1},X_{2},...,X_{k}\}\). These vectors are,

\begin{quote}
\textbf{{[}definition{]}} \emph{linearly dependent} if there exists a
set of scalars \(\{a_{1},a_{2},\dots,a_{k}\}\) such that

\[
a_1X_1 + a_2X_2+\ldots+a_kX_k=0
\]

where at least one \(a_i\neq0\).
\end{quote}

Alternatively, they are,

\begin{quote}
\textbf{{[}definition{]}} \emph{linearly independent} if the only set of
scalars \(\{a_{1},a_{2},\dots,a_{k}\}\) that satisfies the above
condition is \(a_1,a_2,\dots,a_k=0\).
\end{quote}

If we collect these \(k\) column-vectors in a matrix,
\(X=[X_1\;X_2 \dots X_k]\), then the linear dependence condition can be
written as,

\[
a_1X_1 + a_2X_2+\ldots+a_kX_k=\begin{bmatrix} X_1\;X_2 \dots X_k\end{bmatrix}\begin{bmatrix}a_1\\a_2\\\vdots\\a_k\end{bmatrix}=Xa = 0
\]

Given any \(n\times k\) matrix \(X\), its columns are,

\begin{quote}
\textbf{{[}definition{]}} \emph{linearly dependent} if there exists a
vector \(a\in\mathbb{R}^k\) such that \(a\neq0\) and \(Xa=0\);
\end{quote}

or

\begin{quote}
\textbf{{[}definition{]}} \emph{linearly independent} if the only vector
\(a\in\mathbb{R}^k\) such that \(Xa=0\) is \(a=0\).
\end{quote}

For any matrix there may be more than one vector \(a\in\mathbb{R}^{k}\)
such that \(Xa=0\). Indeed, if both \(a_{1},a_{2}\in\mathbb{R}^{k}\)

satisfy this condition and \(a_{1}\neq a_{2}\) then you can show that
any linear combination of \(\{a_{1},a_{2}\}\) satisfies the

condition \(X(a_{1}b_{1}+a_{2}b_{2})=0\) for
\(b_{1},b_{2}\in\mathbb{R}\). Thus, there exists an entire set of
vectors which satisfy this condition. This set is referred to as the,

\begin{quote}
\textbf{{[}definition{]}} \emph{null space} of \(X\),

\[
\mathcal{N}(X) = \{a\in\mathbb{R}^k:\;Xa=0\}
\]
\end{quote}

It should be evident from the definition that if the columns of \(X\)
are linearly independent then \(\mathcal{N}(X)=\{0\}\), a singleton.
That is, it just includes the 0-vector.

\hypertarget{vector-spaces-bases-and-spans}{%
\section{Vector spaces, bases, and
spans}\label{vector-spaces-bases-and-spans}}

Here, we concern ourselves only with real vectors from \(\mathbb{R}^n\).

\begin{quote}
\textbf{{[}definition{]}} A \emph{vector space}, denoted
\(\mathcal{V}\), refers to a set of vectors which is closed under finite
addition and scalar multiplication.
\end{quote}

\begin{quote}
\textbf{{[}definition{]}} A set of \(k\) linearly independent vectors,
\(\{X_1,X_2,\dots,X_k\}\), forms a basis for vector space
\(\mathcal{V}\) if \(\forall\;y\in\mathcal{V}\) there exists a set of
\(k\) scalars such that,

\[
y=X_1b_1+X_2b_2+\ldots+X_kb_k
\]
\end{quote}

Based on these definitions, it is evident that for the Euclidean space,
\(\mathbb{E}^n\), any \(n\) linearly independent vectors from
\(\mathbb{R}^n\) is a basis. For example, any point in \(\mathbb{E}^2\)
can be defined as a multiple of,

\[
\begin{bmatrix}1\\0\end{bmatrix}\quad \text{and} \quad\begin{bmatrix}0\\1\end{bmatrix}
\]

Consider again the \(n\times k\) matrix \(X\), where \(k<n\). Then we
define the,

\begin{quote}
\textbf{{[}definition{]}} \emph{column space} (or \emph{span}) of \(X\),
denoted \(\mathcal{S}(X)\), as the vector space generate by the \(k\)
columns of \(X\). Formally,

\[
\mathcal{S}(X) = \{y\in\mathbb{R}^n:\;y=Xb\quad\text{for some }b\in \mathbb{R}^k\} 
\]
\end{quote}

A property to note about the span or column space \(X\) is,

\begin{quote}
\textbf{{[}result{]}} \(\mathcal{S}(X)=\mathcal{S}(XX')\)
\end{quote}

where \(XX'\) is a \(n\times n\) matrix.

Finally, we can define the,

\begin{quote}
\textbf{{[}definition{]}} \emph{orthogonal column space} (or
\emph{orthogonal} \emph{span}) of \(X\) as,

\[
\mathcal{S}^{\perp}(X) = \{y\in \mathbb{R}^k:\;y'x=0\quad \forall x\in\mathcal{S}(X)\}
\]
\end{quote}

\hypertarget{rank}{%
\section{Rank}\label{rank}}

Consider a \(n\times k\) matrix \(X\), the

\begin{quote}
\textbf{{[}definition{]}} \emph{row rank} of \(X\) is the maximum number
of linearly independent rows:

\[
rowrank(X) \leq n
\]
\end{quote}

We say that matrix \(X\) has \emph{full} row rank if \(rowrank(X)=n\).

The,

\begin{quote}
\textbf{{[}definition{]}} \emph{column rank} of \(X\) is the maximum
number of linearly independent columns:

\[
colrank(X) \leq k
\]
\end{quote}

We say that matrix \(X\) has \emph{full} column rank if
\(colrank(X)=k\).

An important result is,

\begin{quote}
\textbf{{[}result{]}} the rank of \(X\):

\[
r(X) = rowrank(X)=colrank(X) \\
\Rightarrow r(X)\leq min\{n,k\}
\]
\end{quote}

In addition, since the \(r(X)\) depends on the number of linearly
independent columns, we can say that,

\begin{quote}
\textbf{{[}result{]}} the dimension of \(\mathcal{S}(X)\),
\(dim(\mathcal{S}(X))\), is given by the \(r(X)\).
\end{quote}

Here are a few additional results,

\begin{quote}
\textbf{{[}result{]}} \(r(X)=r(X')\)

\textbf{{[}result{]}} \(r(XY)\leq min\{r(X),r(Y)\}\)

\textbf{{[}result{]}} \(r(XY)=r(X)\) if \(Y\) is square and full rank

\textbf{{[}result{]}} \(r(X+Y)\leq r(X) + r(Y)\)
\end{quote}

\hypertarget{properties-of-square-matrices}{%
\section{Properties of square
matrices}\label{properties-of-square-matrices}}

Consider the case of a square, \(n\times n\), matrix \(A\). We say that,

\begin{quote}
\textbf{{[}definition{]}} \(A\) is \emph{singular} if the \(r(A)<n\),
\end{quote}

or that,

\begin{quote}
\textbf{{[}definition{]}} \(A\) is \emph{non-singular} if the
\(r(A)=n\).
\end{quote}

The singularity of a square matrix is important as it determines the
invertibility of a matrix, which typically relates the existence of a
unique solution in systems of linear equations. Here are a few key
results,

\begin{quote}
\textbf{{[}result{]}} There exists a matrix \(B=A^{-1}\), such that
\(AB=I_n\) (where \(I_n\) is the identity matrix), if and only if \(A\)
is non-singular.

\textbf{{[}result{]}} \(A\) is non-singular if and only if the
determinant of \(A\) is non-zero: \(det(A)\neq0\).\footnote{These notes
  do not cover how to calculate the determinant of a square matrix. You
  should be able to find a definition easily online.}

\textbf{{[}result{]}} Likewise, \(A\) is singular if and only if
\(det(A)=0\).

\textbf{{[}result{]}} \(AA^{-1}=A^{-1}A=I\)

\textbf{{[}result{]}} \((A')^{-1}=(A^{-1})'\)

\textbf{{[}result{]}} If their respective inverses exist, then
\((AB)^{-1}=B^{-1}A^{-1}\).

\textbf{{[}result{]}} \(det(AB)=det(A)det(B)\)

\textbf{{[}result{]}} \(det(A^{-1})=det(A)^{-1}\)
\end{quote}

For any square matrix \(A\),

\begin{quote}
\textbf{{[}definition{]}} the \emph{trace} of \(A\) is the sum of all
diagonal elements:

\[
tr(A) = \sum_{i=1}^na_{ii}
\]
\end{quote}

Regarding the trace of a square matrix, here are a few important
results:

\begin{quote}
\textbf{{[}result{]}} \(tr(A+B) = tr(A) + tr(B)\)

\textbf{{[}result{]}} \(tr(\lambda A) = \lambda tr(A)\) where
\(\lambda\) is a scalar

\textbf{{[}result{]}} \(tr(A) = tr(A')\)

\textbf{{[}result{]}} \(tr(AB) = tr(BA)\) where \(AB\) and \(BA\) are
both square, but need not be of the same order.

\textbf{{[}result{]}} \(||A|| = (tr(A'A))^{1/2}\)
\end{quote}

\hypertarget{properties-of-symmetric-matrices}{%
\section{Properties of symmetric
matrices}\label{properties-of-symmetric-matrices}}

A symmetric matrix has the property that \(A=A'\). Therefore, \(A\) must
be square.

Here are a few important results concerning symmetric matrices.

\begin{quote}
\textbf{{[}result{]}} \(A^{-1}\) exists if \(det(A)\neq 0\) and
\(r(A)=n\)

\textbf{{[}result{]}} A is \emph{diagonalizable}.\footnote{A matrix is
  diagonalizable if it is \emph{similar} to some other diagonal matrix.
  Matrices \(B\) and \(C\) are similar if \(C=PBP^{-1}\). A square
  matrix which is not diagonalizable is \emph{defective}. This property
  relates closely to eigenvector decomposition.}

\textbf{{[}result{]}} The eigenvector decomposition of a square matrix
gives you \(A=C\Lambda C^{-1}\) where \(\Lambda\) is a diagonal matrix
of eigenvalues and \$C\$ a matrix of the corresponding eigenvectors. The
symmetry of \(A\) gives you that \(C^{-1}=C'\Rightarrow A=C\Lambda C'\)
with \(C'C=CC'=I_{n}\).\footnote{Recall, an eigenvalue and eigenvector
  pair, \((\lambda,c)\), of matrix \(A\) satisfy:\\
  \[
  Ac = \lambda c\Rightarrow (A-\lambda I_n)c=0
  \]}
\end{quote}

A key definition concerning symmetric matrices is their positive
definiteness:

\begin{quote}
\textbf{{[}definition{]}} \(A\) is \emph{positive semi-definite} if for
any \(x\in\mathbb{R}^n,\;x'Ax\geq0\).
\end{quote}

Given the eigenvector decomposition of a symmetric matrix,
\emph{positive semi-definiteness} implies \(\Lambda\) is \emph{positive
semi-definite}: \(\lambda_i\geq0\quad\forall i\). Likewise,

\begin{quote}
\textbf{{[}definition{]}} \(A\) is \emph{positive definite} if for any
\(x\in\mathbb{R}^n,\;x'Ax>0\).
\end{quote}

Again, based on the egeinvector decomposition, \emph{positive
semi-definiteness} implies \(\Lambda\) is \emph{positive definite}:
\(\lambda_i>0\quad\forall i\).

A few more results are:

\begin{quote}
\textbf{{[}result{]}} \(tr(A) = \sum_{i=1}^n\lambda_i\)

\textbf{{[}result{]}} \(r(A) = r(\Lambda)\)

\textbf{{[}result{]}} \(det(A) = \prod_{i=1}^n \lambda_i\)
\end{quote}

This last result can be used to prove that any positive definite matrix
is non-singular and therefore has an inverse.

Any full-rank, positive semi-definite, symmetric matrix \(B\) has the
additional properties:

\begin{quote}
\textbf{{[}result{]}} \(B=C\Lambda C'\) and \(B^{-1} = C\Lambda^{-1}C'\)

\textbf{{[}result{]}} We can define the square-root of \(B\) as
\(B^{1/2} = C\Lambda^{1/2}C'\). Similarly,
\(B^{-1/2} = C\Lambda^{-1/2}C'\).
\end{quote}

\hypertarget{properties-of-idempotent-matrices}{%
\section{Properties of idempotent
matrices}\label{properties-of-idempotent-matrices}}

An idempotent matrix has the property that \(D=DD\). Therefore, \(D\)
must be square.

Here are a few important results concerning idempotent matrices.

\begin{quote}
\textbf{{[}result{]}} \(D\) is positive definite

\textbf{{[}result{]}} \(D\) is diagonalizable

\textbf{{[}result{]}} \((I_n-D)\) is also an idempotent matrix

\textbf{{[}result{]}} With the exception of \(I_n\), all idempotent
matrices are singular.

\textbf{{[}result{]}} \(r(D) = tr(D) = \sum_{i=1}^n\lambda_i\)

\textbf{{[}result{]}} \(\lambda_i\in\{0,1\}\quad \forall\;i\)
\end{quote}

\emph{Projection} matrices are idempotent, but need not be symmetric.
However, for the purposes of this module we will deal exclusively with
symmetric idempotent projection matrices.



\end{document}
