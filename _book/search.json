[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EC338 Pre-reading",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This e-book provides some pre-reading material for the EC338: Econometrics 2 - Microeconometrics module. In line with the Applied Microeconomics and Microeconometrics literatures, EC338 has evolved over time to focus on causal inference, the identification strategies that underpin various casual estimands, and their corresponding estimators.\nAs a result of this shift in focus, less time is given to more advanced material relating to the linear estimators used in this literature. As the relevant material is covered in EC226: Econometrics 1, you should be able to follow EC338 with a little revision and study. However, the change in notation may ‘trip you up’ and certain topics will be easier to understand with a richer understanding of linear estimators. For this reason, I have prepared this material as a brief (re-)introduction to ordinary least squares (OLS).\nIn EC338 we work extensively with dummy variables. As you know from EC226, estimating a linear regression model with discrete independent variables is relatively simple and the interpretation of the coefficient is typically straight-forward. However, the use of dummy variables requires a careful consideration of (perfect) collinearity and to understand collinearity (or rank conditions) it helps to think of data as a matrix, or system of column vectors. This becomes even more important when we start to consider models with various dimensions of fixed-effects.\nThese notes begin by revisiting the basic linear regression model and OLS estimator using vector notation. Next we revisit the properties of OLS, using this same notation, but without extensive proofs. Proofs should be available from a range of textbooks, including Wooldridge (2011). The material that will be least familiar to you will be the geometry of OLS. Here we will cover projection matrices and how they can be used to understand partitioned regression. Finally, we discuss dummy variables, their projections, and issues of colinearity.\nRemember, this is still the same material covered in EC226, just using vector notation. In many instances this simplifies the notation, as summations over \\(n\\) and/or \\(t\\) can be replaced with a simple inner product of vectors. For example, consider the average of random variable \\(Y\\),\n\\[\n\\bar{Y} = \\frac{1}{n}\\sum_{i=1}^{n}Y_i\n\\]\nIf we define the following two n-dimension column vectors,\n\\[\n\\iota = \\begin{bmatrix}1 \\\\ 1 \\\\ \\vdots \\\\ 1\\end{bmatrix}\\qquad \\text{and}\\qquad Y = \\begin{bmatrix}Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\\end{bmatrix}\n\\]\nthen the inner produce of these vectors is,\n\\[\n\\iota'Y = 1\\cdot Y_1+1\\cdot Y_2+\\dots+1\\cdot Y_n = \\sum_{i=1}^{n}Y_i\n\\]\nIn addition,\n\\[\n\\iota'\\iota = 1\\cdot 1+1\\cdot 1+\\dots+1\\cdot 1 = \\sum_{i=1}^{n}1 = n\n\\]\nThus, the average can be expressed as,\n\\[\n(\\iota'\\iota)^{-1}\\iota'Y = \\frac{1}{n}\\sum_{i=1}^{n}Y_i\n\\]\nIt turns out, this linear transformation of \\(Y\\),\n\\[\nAY = (\\iota'\\iota)^{-1}\\iota'Y\n\\]\nhas a structure that is common to many linear estimators; a structure that derives from the projection matrix of the \\(\\iota\\)-vector: \\(P_{\\iota} =\\iota(\\iota'\\iota)^{-1}\\iota'\\) . We will discuss this in Chapter Chapter 4 when we explore the geometry of OLS.\nAt the end of this e-book is a compendium on linear algebra basics. I may reference these during the e-book. The compendium is not exhaustive and does not include proofs. Please consult a linear algebra text book for further reading if you require.\nThe material in this e-book will not be examined directly, but will assist with your understanding of the material covered during term."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "linear-reg.html",
    "href": "linear-reg.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "properties-ols.html",
    "href": "properties-ols.html",
    "title": "3  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "geometry-ols.html",
    "href": "geometry-ols.html",
    "title": "4  The Geometry of OLS",
    "section": "",
    "text": "In the last section we saw how the OLS estimator can, more generally, be described as a linear transformation of the \\(Y\\) vector.\n\\[\n\\hat{\\beta} = (X'X)^{-1}X'Y\n\\]\nWe also saw that in order for there to be a (unique) solution to the least squared problem, the \\(X\\) matrix must be full rank. This rules out any perfect colinearity between variables in the \\(X\\) matrix, including the constant.\nGiven the vector of OLS coefficients, we can also estimate the residual,\n\\[\n\\begin{align}\n\\hat{\\varepsilon} =& Y - X\\hat{\\beta} \\\\\n=&Y-X(X'X)^{-1}X'Y \\\\\n=&(I_n-X(X'X)X^{-1})Y\n\\end{align}\n\\]\nby plugging the definition of \\(\\hat{\\beta}\\). Thus, the OLS estimator separates the vector \\(Y\\) into two components:\n\\[\n\\begin{align}\nY =& X\\hat{\\beta} + \\hat{\\varepsilon}\\\\\n=&\\underbrace{X(X'X)^{-1}X'}_{P_X}Y + (\\underbrace{I_n-X(X'X)X^{-1}}_{I_n-P_X = M_X})Y \\\\\n=&P_XY + M_XY\n\\end{align}\n\\]\nThe matrix \\(P_X = X(X'X)^{-1}X'\\) is a \\(n\\times n\\) projection matrix. It is a linear transformation that projects any vector into the span of \\(X\\): \\(S(X)\\). (See Chapter 6 more information on these terms.) \\(S(X)\\) is the vector space spanned by the columns of \\(X\\). The dimensions of this vector space depends on the rank of \\(X\\); in this instance, \\(dim(S(X)) = k\\).\nThe matrix \\(M_X = I_n-X(X'X)^{-1}X'\\) is also a \\(n\\times n\\) projection matrix. It projects any vector into \\(X\\)’s orthogonal span"
  },
  {
    "objectID": "dummy-var.html",
    "href": "dummy-var.html",
    "title": "5  Dummy Variables",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "compendium.html",
    "href": "compendium.html",
    "title": "6  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "compendium.html#linear-independence",
    "href": "compendium.html#linear-independence",
    "title": "6  Compendium",
    "section": "6.1 Linear independence",
    "text": "6.1 Linear independence\nConsider a set of \\(k\\) \\(n\\)-dimensional vectors \\(\\{X_{1},X_{2},...,X_{k}\\}\\). These vectors are,\n\n[definition] linearly dependent if there exists a set of scalars \\(\\{a_{1},a_{2},\\dots,a_{k}\\}\\) such that\n\\[\na_1X_1 + a_2X_2+\\ldots+a_kX_k=0\n\\]\nwhere at least one \\(a_i\\neq0\\).\n\nAlternatively, they are,\n\n[definition] linearly independent if the only set of scalars \\(\\{a_{1},a_{2},\\dots,a_{k}\\}\\) that satisfies the above condition is \\(a_1,a_2,\\dots,a_k=0\\).\n\nIf we collect these \\(k\\) column-vectors in a matrix, \\(X=[X_1\\;X_2 \\dots X_k]\\), then the linear dependence condition can be written as,\n\\[\na_1X_1 + a_2X_2+\\ldots+a_kX_k=\\begin{bmatrix} X_1\\;X_2 \\dots X_k\\end{bmatrix}\\begin{bmatrix}a_1\\\\a_2\\\\\\vdots\\\\a_k\\end{bmatrix}=Xa = 0\n\\]\nGiven any \\(n\\times k\\) matrix \\(X\\), its columns are,\n\n[definition] linearly dependent if there exists a vector \\(a\\in\\mathbb{R}^k\\) such that \\(a\\neq0\\) and \\(Xa=0\\);\n\nor\n\n[definition] linearly independent if the only vector \\(a\\in\\mathbb{R}^k\\) such that \\(Xa=0\\) is \\(a=0\\).\n\nFor any matrix there may be more than one vector \\(a\\in\\mathbb{R}^{k}\\) such that \\(Xa=0\\). Indeed, if both \\(a_{1},a_{2}\\in\\mathbb{R}^{k}\\)\nsatisfy this condition and \\(a_{1}\\neq a_{2}\\) then you can show that any linear combination of \\(\\{a_{1},a_{2}\\}\\) satisfies the\ncondition \\(X(a_{1}b_{1}+a_{2}b_{2})=0\\) for \\(b_{1},b_{2}\\in\\mathbb{R}\\). Thus, there exists an entire set of vectors which satisfy this condition. This set is referred to as the,\n\n[definition] null space of \\(X\\),\n\\[\n\\mathcal{N}(X) = \\{a\\in\\mathbb{R}^k:\\;Xa=0\\}\n\\]\n\nIt should be evident from the definition that if the columns of \\(X\\) are linearly independent then \\(\\mathcal{N}(X)=\\{0\\}\\), a singleton. That is, it just includes the 0-vector."
  },
  {
    "objectID": "compendium.html#vector-spaces-bases-and-spans",
    "href": "compendium.html#vector-spaces-bases-and-spans",
    "title": "6  Compendium",
    "section": "6.2 Vector spaces, bases, and spans",
    "text": "6.2 Vector spaces, bases, and spans\nHere, we concern ourselves only with real vectors from \\(\\mathbb{R}^n\\).\n\n[definition] A vector space, denoted \\(\\mathcal{V}\\), refers to a set of vectors which is closed under finite addition and scalar multiplication.\n\n\n[definition] A set of \\(k\\) linearly independent vectors, \\(\\{X_1,X_2,\\dots,X_k\\}\\), forms a basis for vector space \\(\\mathcal{V}\\) if \\(\\forall\\;y\\in\\mathcal{V}\\) there exists a set of \\(k\\) scalars such that,\n\\[\ny=X_1b_1+X_2b_2+\\ldots+X_kb_k\n\\]\n\nBased on these definitions, it is evident that for the Euclidean space, \\(\\mathbb{E}^n\\), any \\(n\\) linearly independent vectors from \\(\\mathbb{R}^n\\) is a basis. For example, any point in \\(\\mathbb{E}^2\\) can be defined as a multiple of,\n\\[\n\\begin{bmatrix}1\\\\0\\end{bmatrix}\\quad \\text{and} \\quad\\begin{bmatrix}0\\\\1\\end{bmatrix}\n\\]\nConsider again the \\(n\\times k\\) matrix \\(X\\), where \\(k&lt;n\\). Then we define the,\n\n[definition] column space (or span) of \\(X\\), denoted \\(\\mathcal{S}(X)\\), as the vector space generate by the \\(k\\) columns of \\(X\\). Formally,\n\\[\n\\mathcal{S}(X) = \\{y\\in\\mathbb{R}^n:\\;y=Xb\\quad\\text{for some }b\\in \\mathbb{R}^k\\}\n\\]\n\nA property to note about the span or column space \\(X\\) is,\n\n[result] \\(\\mathcal{S}(X)=\\mathcal{S}(XX')\\)\n\nwhere \\(XX'\\) is a \\(n\\times n\\) matrix.\nFinally, we can define the,\n\n[definition] orthogonal column space (or orthogonal span) of \\(X\\) as,\n\\[\n\\mathcal{S}^{\\perp}(X) = \\{y\\in \\mathbb{R}^k:\\;y'x=0\\quad \\forall x\\in\\mathcal{S}(X)\\}\n\\]"
  },
  {
    "objectID": "compendium.html#rank",
    "href": "compendium.html#rank",
    "title": "6  Compendium",
    "section": "6.3 Rank",
    "text": "6.3 Rank\nConsider a \\(n\\times k\\) matrix \\(X\\), the\n\n[definition] row rank of \\(X\\) is the maximum number of linearly independent rows:\n\\[\nrowrank(X) \\leq n\n\\]\n\nWe say that matrix \\(X\\) has full row rank if \\(rowrank(X)=n\\).\nThe,\n\n[definition] column rank of \\(X\\) is the maximum number of linearly independent columns:\n\\[\ncolrank(X) \\leq k\n\\]\n\nWe say that matrix \\(X\\) has full column rank if \\(colrank(X)=k\\).\nAn important result is,\n\n[result] the rank of \\(X\\):\n\\[\nr(X) = rowrank(X)=colrank(X) \\\\\n\\Rightarrow r(X)\\leq min\\{n,k\\}\n\\]\n\nIn addition, since the \\(r(X)\\) depends on the number of linearly independent columns, we can say that,\n\n[result] the dimension of \\(\\mathcal{S}(X)\\), \\(dim(\\mathcal{S}(X))\\), is given by the \\(r(X)\\).\n\nHere are a few additional results,\n\n[result] \\(r(X)=r(X')\\)\n[result] \\(r(XY)\\leq min\\{r(X),r(Y)\\}\\)\n[result] \\(r(XY)=r(X)\\) if \\(Y\\) is square and full rank\n[result] \\(r(X+Y)\\leq r(X) + r(Y)\\)"
  },
  {
    "objectID": "compendium.html#properties-of-square-matrices",
    "href": "compendium.html#properties-of-square-matrices",
    "title": "6  Compendium",
    "section": "6.4 Properties of square matrices",
    "text": "6.4 Properties of square matrices\nConsider the case of a square, \\(n\\times n\\), matrix \\(A\\). We say that,\n\n[definition] \\(A\\) is singular if the \\(r(A)&lt;n\\),\n\nor that,\n\n[definition] \\(A\\) is non-singular if the \\(r(A)=n\\).\n\nThe singularity of a square matrix is important as it determines the invertibility of a matrix, which typically relates the existence of a unique solution in systems of linear equations. Here are a few key results,\n\n[result] There exists a matrix \\(B=A^{-1}\\), such that \\(AB=I_n\\) (where \\(I_n\\) is the identity matrix), if and only if \\(A\\) is non-singular.\n[result] \\(A\\) is non-singular if and only if the determinant of \\(A\\) is non-zero: \\(det(A)\\neq0\\).1\n[result] Likewise, \\(A\\) is singular if and only if \\(det(A)=0\\).\n[result] \\(AA^{-1}=A^{-1}A=I\\)\n[result] \\((A')^{-1}=(A^{-1})'\\)\n[result] If their respective inverses exist, then \\((AB)^{-1}=B^{-1}A^{-1}\\).\n[result] \\(det(AB)=det(A)det(B)\\)\n[result] \\(det(A^{-1})=det(A)^{-1}\\)\n\nFor any square matrix \\(A\\),\n\n[definition] the trace of \\(A\\) is the sum of all diagonal elements:\n\\[\ntr(A) = \\sum_{i=1}^na_{ii}\n\\]\n\nRegarding the trace of a square matrix, here are a few important results:\n\n[result] \\(tr(A+B) = tr(A) + tr(B)\\)\n[result] \\(tr(\\lambda A) = \\lambda tr(A)\\) where \\(\\lambda\\) is a scalar\n[result] \\(tr(A) = tr(A')\\)\n[result] \\(tr(AB) = tr(BA)\\) where \\(AB\\) and \\(BA\\) are both square, but need not be of the same order.\n[result] \\(||A|| = (tr(A'A))^{1/2}\\)"
  },
  {
    "objectID": "compendium.html#properties-of-symmetric-matrices",
    "href": "compendium.html#properties-of-symmetric-matrices",
    "title": "6  Compendium",
    "section": "6.5 Properties of symmetric matrices",
    "text": "6.5 Properties of symmetric matrices\nA symmetric matrix has the property that \\(A=A'\\). Therefore, \\(A\\) must be square.\nHere are a few important results concerning symmetric matrices.\n\n[result] \\(A^{-1}\\) exists if \\(det(A)\\neq 0\\) and \\(r(A)=n\\)\n[result] A is diagonalizable.2\n[result] The eigenvector decomposition of a square matrix gives you \\(A=C\\Lambda C^{-1}\\) where \\(\\Lambda\\) is a diagonal matrix of eigenvalues and $C$ a matrix of the corresponding eigenvectors. The symmetry of \\(A\\) gives you that \\(C^{-1}=C'\\Rightarrow A=C\\Lambda C'\\) with \\(C'C=CC'=I_{n}\\).3\n\nA key definition concerning symmetric matrices is their positive definiteness:\n\n[definition] \\(A\\) is positive semi-definite if for any \\(x\\in\\mathbb{R}^n,\\;x'Ax\\geq0\\).\n\nGiven the eigenvector decomposition of a symmetric matrix, positive semi-definiteness implies \\(\\Lambda\\) is positive semi-definite: \\(\\lambda_i\\geq0\\quad\\forall i\\). Likewise,\n\n[definition] \\(A\\) is positive definite if for any \\(x\\in\\mathbb{R}^n,\\;x'Ax&gt;0\\).\n\nAgain, based on the egeinvector decomposition, positive semi-definiteness implies \\(\\Lambda\\) is positive definite: \\(\\lambda_i&gt;0\\quad\\forall i\\).\nA few more results are:\n\n[result] \\(tr(A) = \\sum_{i=1}^n\\lambda_i\\)\n[result] \\(r(A) = r(\\Lambda)\\)\n[result] \\(det(A) = \\prod_{i=1}^n \\lambda_i\\)\n\nThis last result can be used to prove that any positive definite matrix is non-singular and therefore has an inverse.\nAny full-rank, positive semi-definite, symmetric matrix \\(B\\) has the additional properties:\n\n[result] \\(B=C\\Lambda C'\\) and \\(B^{-1} = C\\Lambda^{-1}C'\\)\n[result] We can define the square-root of \\(B\\) as \\(B^{1/2} = C\\Lambda^{1/2}C'\\). Similarly, \\(B^{-1/2} = C\\Lambda^{-1/2}C'\\)."
  },
  {
    "objectID": "compendium.html#properties-of-idempotent-matrices",
    "href": "compendium.html#properties-of-idempotent-matrices",
    "title": "6  Compendium",
    "section": "6.6 Properties of idempotent matrices",
    "text": "6.6 Properties of idempotent matrices\nAn idempotent matrix has the property that \\(D=DD\\). Therefore, \\(D\\) must be square.\nHere are a few important results concerning idempotent matrices.\n\n[result] \\(D\\) is positive definite\n[result] \\(D\\) is diagonalizable\n[result] \\((I_n-D)\\) is also an idempotent matrix\n[result] With the exception of \\(I_n\\), all idempotent matrices are singular.\n[result] \\(r(D) = tr(D) = \\sum_{i=1}^n\\lambda_i\\)\n[result] \\(\\lambda_i\\in\\{0,1\\}\\quad \\forall\\;i\\)\n\nProjection matrices are idempotent, but need not be symmetric. However, for the purposes of this module we will deal exclusively with symmetric idempotent projection matrices."
  },
  {
    "objectID": "compendium.html#footnotes",
    "href": "compendium.html#footnotes",
    "title": "6  Compendium",
    "section": "",
    "text": "These notes do not cover how to calculate the determinant of a square matrix. You should be able to find a definition easily online.↩︎\nA matrix is diagonalizable if it is similar to some other diagonal matrix. Matrices \\(B\\) and \\(C\\) are similar if \\(C=PBP^{-1}\\). A square matrix which is not diagonalizable is defective. This property relates closely to eigenvector decomposition.↩︎\nRecall, an eigenvalue and eigenvector pair, \\((\\lambda,c)\\), of matrix \\(A\\) satisfy:\n\\[\nAc = \\lambda c\\Rightarrow (A-\\lambda I_n)c=0\n\\]↩︎"
  },
  {
    "objectID": "linear-reg.html#vector-notation",
    "href": "linear-reg.html#vector-notation",
    "title": "2  The Linear Regression Model",
    "section": "2.1 Vector notation",
    "text": "2.1 Vector notation\nMost undergraduate textbooks discuss data in terms of random variables: the dependent (outcome) variable (\\(Y\\)) and various independent (explanatory) variables (\\(X_1,X_2,\\ldots,X_k\\)). There’s nothing wrong with this language, but to understand the geometry of OLS we will need to think in random vectors.\nThink about the random data as a collection of observations for unit \\(i=1,\\ldots,n\\). For each unit you observe the outcome (\\(Y_i\\)) and a vector of explanatory variables (or regressors):\n\\[\nX_i = \\begin{bmatrix}X_{i1} \\\\ X_{i2} \\\\ \\vdots \\\\ X_{ik}\\end{bmatrix}\n\\]\nTake note of the ordering of the subscripts: the first denotes the unit of observation (\\(i\\)) and the second the number of the regressor (\\(k\\)). The pair \\((Y_i,X_i)\\) reflects an observation, where \\(Y_i\\) is a single random variable and \\(X_i\\) a random (column) vector.1 A collection of observations forms a sample.\nYou are, no doubt, familiar with the linear regression model. A simple univariate model is typically written as,\n\\[\nY_i = \\beta_1 + \\beta_2 X_{i2} + \\varepsilon_i\n\\]\nWhere \\(\\beta_1\\) is the constant (intercept) and \\(\\beta_2\\) the slope coefficient. In EC338, we will discuss in more detail the justification for the model. For now, let us focus on notation.\nThis linear regression model is linear in parameters, which means we can write as a linear function of a set of \\(\\beta\\)-parameters. These parameters are constant and unknown. Here is how we could rewrite the above equation using vectors,\n\\[\n\\begin{align} Y_i &= \\begin{bmatrix}1\\;X_{i2}\\end{bmatrix}\\begin{bmatrix}\\beta_1 \\\\ \\beta_2\\end{bmatrix} + \\varepsilon_i \\\\\n&= \\begin{bmatrix}1\\\\ X_{i2}\\end{bmatrix}'\\begin{bmatrix}\\beta_1 \\\\ \\beta_2\\end{bmatrix} + \\varepsilon_i \\\\\n&=X_i'\\beta + \\varepsilon_i\n\\end{align}\n\\]\nWhere \\(X_i\\) is a column vector including the number 1 in the first row (for the constant/intercept) and \\(X_{i2}\\) in the second row.\n\n[important] You find my notation slightly unusual. First of all, a lot undergraduate textbooks use different letters (\\(\\alpha\\), \\(\\beta\\), \\(\\gamma\\), etc.) to denote the constant and slope coefficient(s). Since we want to collect all of the coefficients into a single vector it makes sense to use indexes instead of different letters.\nThen there is the issue of the where to start the index: \\(0\\) or \\(1\\). This decision is somewhat arbitrary and hinges on whether the the \\(k\\) regressors included in the model includes the constant term. At the top of the page I described an observation as an outcome and a random vector of regressors. As the constant is not a random variable it is natural to think of it apart from the set of regressors included in the model. However, this decision is somewhat arbitrary. You could simply set \\(X_{i1} = 1\\;\\forall\\;i\\) and the constant would be included in \\(k\\).\nIn EC226, you indexed from 0. Recall, the linear model as \\(k+1\\) parameters; the \\(+1\\) for the constant. When computing the degrees of freedom for the RSS, you solved for \\(n-k-1\\): \\(k\\) regressors plus the constant.\nHere we will index from \\(1\\). This distinction makes it easier to keep track of the size of the matrix. It is also a more natural notation if you consider that the model need not have a constant. The choice of including a constant is therefore no different to including any other regressor. Moreover, when we consider models with fixed effects the constant typically drops from the model.\nThe key thing to remember is that you need to keep track of the number of parameters in the model, that includes the constant if there is one.\n\nWe can easily extend this notation to the case of multivariate regression. For example, consider a model with a constant and \\(k-1\\) regressors.\n\\[\n\\begin{align} Y_i &= \\beta_1 + \\beta_2 X_{i2} + \\beta_3 X_{i3} + \\ldots + \\beta_k X_{ik} + \\varepsilon_i \\\\\n&= \\begin{bmatrix} 1\\; X_{i2}\\; X_{i3}\\; \\dots\\; X_{ik} \\end{bmatrix} \\begin{bmatrix} \\beta_1\\\\ \\beta_2\\\\ \\beta_3\\\\ \\vdots\\\\ \\beta_k\\end{bmatrix} + \\varepsilon_i \\\\\n&=X_i'\\beta + \\varepsilon_i\n\\end{align}\n\\]\nRegardless of the number of regressors, the notation remains the same. Take note of the fact that the \\(X_i\\) is a column vector which means that the notation must include a transpose: \\(X_i'\\beta\\). Excluding the transpose is incorrect since you cannot multiple two \\(k\\times1\\) column vectors. You can multiply a \\(1\\times k\\) row vector with a \\(k\\times 1\\) column vector giving you a \\(1\\times 1\\) scalar. The result should be a scalar since \\(Y_i\\) is a scalar.\nThis notation is not universal. For example, Wooldridge (2011) treats \\(X_i\\) as a row vector. For this reason, the linear regression model can be expressed as \\(X_i\\beta\\). Both notations are used in the applied literature, but I am more familiar with and prefer the column-vector notation."
  },
  {
    "objectID": "linear-reg.html#matrix-notation",
    "href": "linear-reg.html#matrix-notation",
    "title": "2  The Linear Regression Model",
    "section": "2.2 Matrix notation",
    "text": "2.2 Matrix notation\nThe above expressions for the linear regression model all describe a single unit of observation from the sample. Consider, each line included the subscript \\(i\\). Since the model is assumed to be the same for each observation, this an accurate depiction of the linear regression model. However, we also need to think about the correct notation for the entire sample. To do so, we will have to worker with both vectors and matrices.\nSince the model is the same for each observation in the sample, we could imagine “stacking” all \\(n\\) observations on top of one another to form a vector. Consider first the outcome variable,\n\\[\nY=\\begin{bmatrix}Y_1\\\\ Y_2 \\\\ \\vdots\\\\ Y_n \\end{bmatrix}\n\\]\n$$\n$$\n\\(Y\\) is a \\(n\\times 1\\) column vector of all outcomes in the sample. You can distinguish the vector \\(Y\\) from the scalar \\(Y_i\\) by the absence of a subscript.\nSimilarly, we can stack the right-hand side of the equation.\n\\[\n\\begin{align}\nY&=\\begin{bmatrix}X_1'\\beta\\\\ X_2'\\beta \\\\ \\vdots\\\\ X_n'\\beta \\end{bmatrix}+ \\begin{bmatrix}\\varepsilon_1\\\\ \\varepsilon_2 \\\\ \\vdots\\\\ \\varepsilon_n \\end{bmatrix}\\\\\n&=\\begin{bmatrix}X_1'\\\\ X_2' \\\\ \\vdots\\\\ X_n' \\end{bmatrix}\\beta+ \\begin{bmatrix}\\varepsilon_1\\\\ \\varepsilon_2 \\\\ \\vdots\\\\ \\varepsilon_n \\end{bmatrix}\\\\\n&=X\\beta+\\varepsilon\n\\end{align}\n\\]\nLike \\(Y\\), \\(\\varepsilon\\) is a \\(n\\times 1\\) vector. \\(X\\) is a \\(n\\times k\\) matrix and \\(\\beta\\) remains a \\(k\\times 1\\) vector of parameters. The product of a \\(n\\times k\\) matrix and \\(k\\times 1\\) vector is a \\(n\\times 1\\) vector: the same size vector as \\(Y\\) and \\(\\varepsilon\\). As it is important to understand the structure of \\(X\\), let us write it out in detail.\n\\[\nX = \\begin{bmatrix}X_1'\\\\ X_2' \\\\ \\vdots\\\\ X_n' \\end{bmatrix} = \\begin{bmatrix}X_{11} & X_{12}&\\dots&X_{1k}\\\\ X_{21}& X_{22}& & \\\\ \\vdots & & \\ddots &\\\\ X_{n1} & & & X_{nk} \\end{bmatrix}\n\\] The \\(X\\) matrix has \\(n\\) rows, each representing a different unit of observation, and \\(k\\) columns, each representing a different regressor. Recall, one of these regressors may be a constant. If the model includes a constant then \\(X_{i1}=1\\;\\forall\\;i\\). This means that the first column of \\(X\\) is a vector of \\(1\\)’s. Each subsequent column represents another regressor.\nIf you are familiar with rectangular datasets from with working in STATA or R, then you may have notices that \\(X\\) is essentially the “dataset” (excluding the outcome variable). In a rectangular dataset, each row represents a different observation and each column a different variable. That’s what we have here.\nWhy is this noteworthy? When assert that there must be an absence of perfect colinearity between the variables in the model, we are actually saying that the columns of \\(X\\) must be linearly independent. The formal way of expressing this is that \\(X\\) must have full column rank; or \\(r(X)=k\\) (see Chapter 6 for a definition of linear dependence and rank). This is why the OLS condition included in EC226 as “no perfect colinearity” is sometimes referred to as the rank condition. Without full rank, we cannot estimate the linear regression model."
  },
  {
    "objectID": "linear-reg.html#footnotes",
    "href": "linear-reg.html#footnotes",
    "title": "2  The Linear Regression Model",
    "section": "",
    "text": "In these notes, as in the remainder of EC338, I treat \\(X_i\\) as a column vector. Some texts, including Wooldridge (2011), will treat \\(X_i\\) as a row vector. This distinction is not significant, but will affect your notation. I will point this out at a later stage.↩︎"
  },
  {
    "objectID": "ols.html#the-univariate-case",
    "href": "ols.html#the-univariate-case",
    "title": "3  Ordinary Least Squares",
    "section": "3.1 The univariate case",
    "text": "3.1 The univariate case"
  },
  {
    "objectID": "ols.html#the-uni-variate-case",
    "href": "ols.html#the-uni-variate-case",
    "title": "3  Ordinary Least Squares",
    "section": "3.1 The uni-variate case",
    "text": "3.1 The uni-variate case\nUndergraduate textbooks all teach a very similar expression for the OLS estimator of a uni-variate regression model (with a constant), such as\n\\[\nY_i = \\beta_1+\\beta_2X_{i2}+\\varepsilon_i\n\\]\n\n[note] Once you are familiar with vector notation, it is relatively easy to tell whether a model is uni- or multi-variate. This is because the notation \\(\\beta_2 X_{i2}\\) is not consistent with \\(X_{2i}\\) being a vector (row or column).\nIf \\(X_{i2}\\) is a \\(k\\times 1\\) vector then so is \\(\\beta_2\\). Thus, \\(\\beta_2 X_{i2}\\) is \\((k\\times 1)\\cdot (k\\times1)\\), which is not defined.\nIf \\(X_{i2}\\) is a row vector (as in Wooldridge, 2011), \\(\\beta_2 X_{i2}\\) will then be \\((k\\times 1)\\cdot (1\\times k)\\), a \\(k\\times k\\) matrix. This cannot be correct since the model is defined at the unit level.\nThus, if you see a model written with the parameter in front of the regressor, you know that this must be a single regressor. This is subtle, yet imporant, distinction that researchers often use to convey the structure of their model. Whenever \\(X_{i2}\\) is a vector, researchers will almost always use the notation \\(X_{i2}'\\beta\\) or \\(X_{i2}\\beta\\), depending on whether \\(X_{i2}\\) is assumed to be a column or row vector.\n\nWe know that,\n\\[\n\\begin{align}\n\\tilde{\\beta}_2 =& \\frac{\\sum(Y_i-\\bar{Y})(X_{i2}-\\bar{X}_2)}{\\sum_{i=1}^n(X_{i2}-\\bar{X}_2)^2} \\\\\n\\text{and}\\qquad \\tilde{\\beta}_1 =& \\bar{Y}-\\tilde{\\beta_2}\\bar{X}_2\n\\end{align}\n\\]\nI am deliberately using the notation \\(\\tilde{\\beta}\\) to distinguish these two estimators from the expression below.\nLet us see if we can replicate this result. When written in vector notation, the model is,\n\\[\n\\begin{align}\nY =& X\\beta+\\varepsilon \\\\\n=& \\begin{bmatrix}1&X_{12} \\\\ 1 & X_{22} \\\\ \\vdots & \\vdots \\\\ 1 & X_{n2}\\end{bmatrix}\\begin{bmatrix}\\beta_1 \\\\ \\beta_2 \\end{bmatrix} + \\varepsilon \\\\\n=& \\begin{bmatrix}\\iota &X_{2} \\end{bmatrix}\\begin{bmatrix}\\beta_1 \\\\ \\beta_2 \\end{bmatrix} + \\varepsilon\n\\end{align}\n\\]\nTherefore,\n\\[\n\\begin{align}\n\\hat{\\beta} = \\begin{bmatrix}\\hat{\\beta}_1 \\\\ \\hat{\\beta}_2\\end{bmatrix}=&(X'X)^{-1}X'Y \\\\\n=&\\bigg(\\begin{bmatrix}\\iota' \\\\ X_{2}' \\end{bmatrix}\\begin{bmatrix}\\iota &X_{2}\\end{bmatrix}\\bigg)^{-1}\\begin{bmatrix}\\iota'  \\\\ X_{2}' \\end{bmatrix}Y \\\\\n=&\\begin{bmatrix}\\iota'\\iota & \\iota'X_2 \\\\ X_{2}'\\iota & X_2'X_2 \\end{bmatrix}^{-1}\\begin{bmatrix}\\iota'Y  \\\\ X_{2}'Y \\end{bmatrix}\n\\end{align}\n\\]\nI went through this rather quickly, using a number of linear algebra rules that you may not be familiar with. Do not worry, the point of the exercise is not become a linear algebra master, but instead to focus on the element of each of each matrix/vector. Each element is a scalar (size \\(1\\times 1\\)).\nIf we right them each down as sums you they might be a little more familiar. (Take a look back at Chapter 1 to remind yourself of some of these steps). First consider the \\(2\\times 2\\) matrix:\n\nelement [1,1]: \\(\\iota'\\iota = \\sum_{i=1}^n 1 = n\\)\nelement [1,2]: \\(\\iota'X_2 = \\sum_{i=1}^nX_{i2} = n\\bar{X}_2\\)\nelement [2,1]: \\(X_2'\\iota = \\sum_{i=1}^nX_{i2} = n\\bar{X}_2\\) (as above, since scalars are symmetric)\nelement [2,2]: \\(X_2'X_2=\\sum_{i=1}^nX_{i2}^2\\)\n\nNext, consider the final \\(2\\times 1\\) vector,\n\nelement [1,1]: \\(\\iota'Y = \\sum_{i=1}^n Y_i = n\\bar{Y}\\)\nelement [2,1]: \\(X_2'Y = \\sum_{i=1}^nY_iX_{i2}\\)\n\nOur OLS estimator is therefore,\n\\[\n\\hat{\\beta} = \\begin{bmatrix} n & n\\bar{X}_2 \\\\ n\\bar{X}_2 & \\sum_{i=1}^nX_{i2}^2 \\end{bmatrix}^{-1}\\begin{bmatrix}n\\bar{Y}_2  \\\\ \\sum_{i=1}^nY_iX_{i2} \\end{bmatrix}\n\\]\nWe now need to solve for the inverse of the \\(2\\times 2\\) matrix. You can easily find notes on how to do this online. Here, I will just provide the solution.\n\\[\n\\hat{\\beta} = \\frac{1}{n\\sum_{i=1}^nX_{i2}^2-n^2\\bar{X}_2^2}\\begin{bmatrix} \\sum_{i=1}^nX_{i2}^2 & -n\\bar{X}_2 \\\\ -n\\bar{X}_2 &  n\\end{bmatrix}\\begin{bmatrix}n\\bar{Y}_2  \\\\ \\sum_{i=1}^nY_iX_{i2} \\end{bmatrix}\n\\]\nRemember, this is still a \\(2\\times 1\\) vector. We can now solve for the final solution:\n\\[\n\\begin{align}\n\\hat{\\beta} =& \\frac{1}{n\\sum_{i=1}^nX_{i2}^2-n^2\\bar{X}_2^2}\\begin{bmatrix} n\\bar{Y}\\sum_{i=1}^nX_{i2}^2 -n\\bar{X}_2\\sum_{i=1}^nY_iX_{i2} \\\\ n\\sum_{i=1}^nY_iX_{i2}-n^2\\bar{X}_2\\bar{Y}\\end{bmatrix} \\\\\n=& \\frac{1}{n\\sum_{i=1}^n(X_{i2}-\\bar{X}_2)^2}\\begin{bmatrix} n\\bar{Y}\\sum_{i=1}^nX_{i2}^2 + n^2\\bar{Y}\\bar{X}^2 - n^2\\bar{Y}\\bar{X}^2 -n\\bar{X}_2\\sum_{i=1}^nY_iX_{i2} \\\\ n\\sum_{i=1}^n(Y_i-\\bar{Y})(X_{i2}-\\bar{X}_2) \\end{bmatrix} \\\\\n=& \\frac{1}{n\\sum_{i=1}^n(X_{i2}-\\bar{X}_2)^2}\\begin{bmatrix} n\\bar{Y}\\sum_{i=1}^n(X_{i2}-\\bar{X})^2 -n\\bar{X}_2\\sum_{i=1}^n(Y_i-\\bar{Y})(X_{i2}-\\bar{X}_2) \\\\ n\\sum_{i=1}^n(Y_i-\\bar{Y})(X_{i2}-\\bar{X}_2) \\end{bmatrix} \\\\\n=& \\begin{bmatrix} \\bar{Y}  -\\frac{n\\sum_{i=1}^n(Y_i-\\bar{Y})(X_{i2}-\\bar{X}_2)}{n\\sum_{i=1}^n(X_{i2}-\\bar{X}_2)^2}\\bar{X}_2 \\\\ \\frac{n\\sum_{i=1}^n(Y_i-\\bar{Y})(X_{i2}-\\bar{X}_2)}{n\\sum_{i=1}^n(X_{i2}-\\bar{X}_2)^2} \\end{bmatrix} \\\\\n=& \\begin{bmatrix} \\bar{Y}  -\\tilde{\\beta}_2\\bar{X}_2 \\\\ \\tilde{\\beta}_2 \\end{bmatrix} \\\\\n=& \\begin{bmatrix}\\tilde{\\beta}_1 \\\\ \\tilde{\\beta}_2 \\end{bmatrix}\n\\end{align}\n\\]\nThe math is a little involved, but it shows you these solutions are are the same. Unfortunately, the working gets even more arduous in a multivariate context. However, there are useful tools to help us with that we will discuss next."
  },
  {
    "objectID": "geometry-ols.html#partitioned-regression",
    "href": "geometry-ols.html#partitioned-regression",
    "title": "4  The Geometry of OLS",
    "section": "4.1 Partitioned regression",
    "text": "4.1 Partitioned regression\nThe tools of linear algebra also help us to understand partitioned regression. In fact, I would go as far to to say that it is quite difficult to understand partitioned regression without an understanding of projection matrices. Moreover, we need to understand partitioned regression to really understand multivariate regression.\nLet us divide the set of regressors into two groups: \\(X_1\\) a single regressor and \\(X_2\\) a \\(n\\times (k-1)\\) matrix. We can rewrite the linear model as,\n\\[\nY = X\\beta+ \\varepsilon =\\beta_1X_1+X_2\\beta_2+\\varepsilon\n\\]\nPartitioned regression is typically taught as follows. The OLS estimator for \\(\\beta_1\\) can achieved by first regressing \\(X_1\\) on \\(X_2\\),\n\\[\nX_1 = X_2\\gamma_2+\\upsilon_1\n\\]\nNext, you regress \\(Y\\) on the residual from the above model,\n\\[\nY = \\gamma_1 \\hat{\\upsilon}_1+\\xi\n\\]\nThe OLS estimator for \\(\\gamma_1\\) is equal to that of \\(\\beta_1\\).\nLet us begin by applying our existing knowledge. From above, we know that the residual from the regression of \\(X_1\\) on \\(X_2\\) is,\n\\[\n\\hat{\\upsilon} = M_2X_1\n\\]\nwhere \\(M_2 = I_n-X_2(X_2'X_2)^{-1}X_2'\\). Thus, the model we estimate in the second step, is\n\\[\nY = \\gamma_1M_2X_1+\\xi\n\\]\nWe know that \\(\\hat{\\gamma}_1 = (\\hat{\\upsilon}'\\hat{\\upsilon})^{-1}\\hat{\\upsilon}'Y\\). Replacing the value of the residual, we get\n\\[\n\\begin{align}\n\\hat{\\gamma}_1 =& (\\hat{\\upsilon}'\\hat{\\upsilon})^{-1}\\hat{\\upsilon}'Y \\\\\n=&(X_1'M_2M_2X_1)^{-1}X_1'M_2Y \\\\\n=&(X_1'M_2X_1)^{-1}X_1'M_2Y\n\\end{align}\n\\] &gt; [note] We use both the symmetry and idempotent quality of \\(M_2\\).\nNext we want to show that \\(\\hat{\\beta}_1\\) is given by the same value. This part is more complicated. Let’s start with by reminding ourselves of the following:\n\\[\n\\begin{align}\nX'X\\hat{\\beta} &= X'Y \\\\\n\\begin{bmatrix}X_1 & X_2\\end{bmatrix}'\\begin{bmatrix}X_1 & X_2\\end{bmatrix}\\begin{bmatrix}\\hat{\\beta}_1 \\\\ \\hat{\\beta}_2\\end{bmatrix} &= \\begin{bmatrix}X_1 & X_2\\end{bmatrix}'Y \\\\\n\\begin{bmatrix}X_1'X_1 & X_1'X_2 \\\\ X_2'X_1 & X_2'X_2 \\end{bmatrix}\\begin{bmatrix}\\hat{\\beta}_1 \\\\ \\hat{\\beta}_2\\end{bmatrix} &= \\begin{bmatrix}X_1'Y \\\\ X_2'Y\\end{bmatrix}\n\\end{align}\n\\] We could solve for \\(\\hat{\\beta}_1\\) by solving for the inverse of \\(X'X\\); however, this will take a long time. An easier approach is to simply verify that \\(\\hat{\\beta}_1=(X_1'M_2X_1)^{-1}X_1'M_2Y\\). Recall, \\(\\hat{\\beta}\\) splits \\(Y\\) into two components:\n\\[\nY = \\hat{\\beta}_1X_1+X_2\\hat{\\beta}_2 + \\hat{\\varepsilon}\n\\]\nIf we plug this definition of \\(Y\\) into the above expression we get,\n\\[\n\\begin{align}\n&(X_1'M_2X_1)^{-1}X_1'M_2(\\hat{\\beta}_1X_1+X_2\\hat{\\beta}_2 + \\hat{\\varepsilon}) \\\\\n=&\\hat{\\beta}_1\\underbrace{(X_1'M_2X_1)^{-1}X_1'M_2X_1}_{=I_n} \\\\\n&+\\underbrace{(X_1'M_2X_1)^{-1}X_1'M_2X_2\\hat{\\beta_2}}_{=0} \\\\\n&+\\underbrace{(X_1'M_2X_1)^{-1}X_1'M_2\\hat{\\varepsilon}}_{=0} \\\\\n=&\\hat{\\beta}_1\n\\end{align}\n\\] In line 2 I use the fact that \\(\\hat{\\beta}_1\\) is a scalar and can be moved to the front (order of multiplication does not matter). In line 3 we exploit the fact that \\(M_2X_2=0\\) by definition. Line 4 uses the fact that \\(M_2\\hat{\\varepsilon}=\\hat{\\varepsilon}\\) which means that \\(X_1'M_2\\hat{\\varepsilon}=X_1'\\hat{\\varepsilon}=0\\)."
  }
]